{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579c21af",
   "metadata": {},
   "source": [
    "# üß† Brain Tumor Segmentation with U-Net & Attention U-Net (Showcase Notebook)\n",
    "**Internship Project | Deep Learning | PyTorch**\n",
    "\n",
    "---\n",
    "This notebook demonstrates a full pipeline for brain tumor segmentation using U-Net and Attention U-Net.\n",
    "Includes full training loop, Dice metric, validation, visualizations, early stopping, learning rate scheduling,\n",
    "and export options for deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c2d16",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d69666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('‚úÖ Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a955e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA available: True\n",
      "üß† Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"‚úÖ CUDA available:\", torch.cuda.is_available())\n",
    "print(\"üß† Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44235593",
   "metadata": {},
   "source": [
    "## üìÇ Dataset: 4-Channel MRI Loader (.npy format)\n",
    "Each image is a 4-channel MRI slice saved as `.npy` (shape: HxWx4)\n",
    "\n",
    "Each mask is a 2D array (HxW) with integer labels (e.g., 0 = background, 1 = tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d72b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "\n",
    "        if not os.path.isdir(image_dir):\n",
    "            raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "        if not os.path.isdir(mask_dir):\n",
    "            raise FileNotFoundError(f\"Mask directory not found: {mask_dir}\")\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".npy\")])\n",
    "        self.mask_files  = sorted([f.replace(\"image_\", \"mask_\") for f in self.image_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path  = os.path.join(self.mask_dir,  self.mask_files[idx])\n",
    "\n",
    "        image = np.load(image_path)  # shape: (C, D, H, W) or (D, H, W, C) or similar\n",
    "        mask  = np.load(mask_path)   # shape: (D, H, W)\n",
    "\n",
    "        # If image is (D, H, W, C), move channels to front\n",
    "        if image.shape[-1] == 4:     # (D, H, W, C)\n",
    "            image = np.moveaxis(image, -1, 0)  # (C, D, H, W)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        mask  = torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e02d3e",
   "metadata": {},
   "source": [
    "## üß† U-Net 3D and Attention U-Net3D Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a744af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3, features=[32, 64, 128, 256]):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.encoder.append(self._block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        self.bottleneck = self._block(features[-1], features[-1]*2)\n",
    "        for feature in reversed(features):\n",
    "            self.decoder.append(nn.ConvTranspose3d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.decoder.append(self._block(feature*2, feature))\n",
    "        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        skips = skips[::-1]\n",
    "        for i in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[i](x)\n",
    "            skip = skips[i//2]\n",
    "            if x.shape != skip.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip.shape[2:])\n",
    "            x = torch.cat((skip, x), dim=1)\n",
    "            x = self.decoder[i+1](x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "    def _block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class AttentionBlock3D(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock3D, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv3d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm3d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv3d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm3d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv3d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class AttentionUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3, features=[32, 64, 128, 256]):\n",
    "        super(AttentionUNet3D, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.encoder.append(self._block(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        self.bottleneck = self._block(features[-1], features[-1]*2)\n",
    "        for feature in reversed(features):\n",
    "            self.decoder.append(nn.ConvTranspose3d(feature*2, feature, kernel_size=2, stride=2))\n",
    "            self.attentions.append(AttentionBlock3D(F_g=feature, F_l=feature, F_int=feature//2))\n",
    "            self.decoder.append(self._block(feature*2, feature))\n",
    "        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        skips = skips[::-1]\n",
    "        for i in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[i](x)\n",
    "            skip = skips[i//2]\n",
    "            if x.shape != skip.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip.shape[2:])\n",
    "            skip = self.attentions[i//2](g=x, x=skip)\n",
    "            x = torch.cat((skip, x), dim=1)\n",
    "            x = self.decoder[i+1](x)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "    def _block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823361e3",
   "metadata": {},
   "source": [
    "## üîÄ Dataset Split: Train & Validation\n",
    "We use an 80/20 split to train and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27317d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2547\n",
      "Image shape: torch.Size([4, 192, 192, 128])\n",
      "Mask shape: torch.Size([192, 192, 128])\n"
     ]
    }
   ],
   "source": [
    "dataset = BrainTumorDataset(\n",
    "    r\"D:\\Khedir-meriem-ESI-SBElAbes\\data\\input_data_4channels_z_score\\train\\images\",\n",
    "    r\"D:\\Khedir-meriem-ESI-SBElAbes\\data\\input_data_4channels_z_score\\train\\masks\",\n",
    ")\n",
    "\n",
    "print(\"Total samples:\", len(dataset))\n",
    "img, mask = dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,            # Use 2 for 3D models (adjust if OOM or slow)\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,            # Same as above\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4144854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1ed8a",
   "metadata": {},
   "source": [
    "## üìê Dice Coefficient Metric\n",
    "Used to evaluate how well the predicted segmentation matches the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97e4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(pred, target, eps=1e-6):\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    intersection = (pred == target).float().sum()\n",
    "    dice = (2. * intersection) / (pred.numel() + target.numel() + eps)\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c48c9",
   "metadata": {},
   "source": [
    "## üß™ Training Function (AMP + EarlyStopping + Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e013f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import csv\n",
    "\n",
    "def train_model(model, name):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    best_dice = 0\n",
    "    patience = 10\n",
    "    trigger = 0\n",
    "    train_dice, val_dice = [], []\n",
    "\n",
    "    # Prepare CSV log file\n",
    "    log_path = f\"{name}_log.csv\"\n",
    "    with open(log_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"TrainLoss\", \"ValDice\"])\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dice = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with autocast():\n",
    "                    pred = torch.argmax(model(x), dim=1)\n",
    "                dice += dice_coeff(pred.cpu(), y.cpu())\n",
    "        avg_dice = dice / len(val_loader)\n",
    "        val_dice.append(avg_dice.item())\n",
    "\n",
    "        # Log to CSV\n",
    "        with open(log_path, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, total_loss, avg_dice.item()])\n",
    "\n",
    "        print(f\"[{name}] Epoch {epoch}, Loss: {total_loss:.4f}, Val Dice: {avg_dice:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "            print(f\"‚úÖ Saved best model with Dice: {best_dice:.4f}\")\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # After training: plot Dice scores\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(val_dice, label='Validation Dice')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Dice Coefficient\")\n",
    "    plt.title(f\"Validation Dice Curve ({name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{name}_dice_plot.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a024cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Is model on GPU? True\n"
     ]
    }
   ],
   "source": [
    "model = UNet3D().to(DEVICE)\n",
    "print(\"üî• Is model on GPU?\", next(model.parameters()).is_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d94a0",
   "metadata": {},
   "source": [
    "## üß† Train Both Models (100 Epochs Max + Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2037/2037 [26:11<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unet3d] Epoch 1, Loss: 396.3700, Val Dice: 0.9923\n",
      "‚úÖ Saved best model with Dice: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2037/2037 [25:45<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[unet3d] Epoch 2, Loss: 55.0269, Val Dice: 0.9938\n",
      "‚úÖ Saved best model with Dice: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   4%|‚ñç         | 83/2037 [01:03<24:56,  1.31it/s]"
     ]
    }
   ],
   "source": [
    "# Make sure the cell defining train_model is run before this cell.\n",
    "train_model(UNet3D(), \"unet3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that finishes\n",
    "train_model(AttentionUNet3D(), \"att_unet3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee0506",
   "metadata": {},
   "source": [
    "## üìä Plot Dice Score Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_size, label='Train Dice')\n",
    "plt.plot(val_size, label='Val Dice')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice')\n",
    "plt.title('Training Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b165cb",
   "metadata": {},
   "source": [
    "## üîç Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model_path, model_class):\n",
    "    model = model_class().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "    # Show first slice of first 3 volumes\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 3, i*3+1); plt.imshow(x[i][0, 0].cpu(), cmap='gray'); plt.title('Input (slice 0)')\n",
    "        plt.subplot(3, 3, i*3+2); plt.imshow(y[i][0].cpu(), cmap='gray'); plt.title('GT (slice 0)')\n",
    "        plt.subplot(3, 3, i*3+3); plt.imshow(pred[i][0].cpu(), cmap='gray'); plt.title('Prediction (slice 0)')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "visualize_predictions(\"unet3d_best.pth\", UNet3D)\n",
    "visualize_predictions(\"att_unet3d_best.pth\", AttentionUNet3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c20f766",
   "metadata": {},
   "source": [
    "## üíæ Export Models: TorchScript + ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(1, 4, 64, 128, 128).to(DEVICE)  # Adjust shape to your 3D input\n",
    "model = AttentionUNet3D().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"att_unet3d_best.pth\"))\n",
    "torch.jit.trace(model, dummy).save(\"attention_unet3d.pt\")\n",
    "torch.onnx.export(model, dummy, \"attention_unet3d.onnx\", input_names=[\"input\"], output_names=[\"output\"], opset_version=11)\n",
    "print(\"‚úÖ Exported to .pt and .onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26ebf2",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary & Deployment Tips\n",
    "- Best Dice model saved as `.pth`\n",
    "- Deployment-ready formats: `.pt` (TorchScript), `.onnx`\n",
    "- Use ONNX for ONNX Runtime, TensorRT, or OpenVINO.\n",
    "- Ideal for mobile, embedded, or web deployment.\n",
    "\n",
    "**Next step:** Try converting the ONNX model into TensorRT or integrating in a simple Flask demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dd79a",
   "metadata": {},
   "source": [
    "## üß™ Advanced Data Augmentation with Albumentations\n",
    "We use Albumentations to apply more realistic and effective augmentations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "class AugmentedBrainTumorDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augment=False):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, '*.npy')))\n",
    "        self.mask_paths = sorted(glob(os.path.join(mask_dir, '*.npy')))\n",
    "        self.augment = augment\n",
    "        self.transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.image_paths[idx]).astype(np.float32)\n",
    "        mask = np.load(self.mask_paths[idx]).astype(np.uint8)\n",
    "        image = image.transpose(2, 0, 1)  # [C, H, W]\n",
    "        if self.augment:\n",
    "            augmented = self.transform(image=image.transpose(1, 2, 0), mask=mask)\n",
    "            image, mask = augmented['image'], augmented['mask']\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32)\n",
    "            mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace old dataset and reload\n",
    "dataset = AugmentedBrainTumorDataset(\n",
    "    r\"D:\\Khedir-meriem-ESI-SBElAbes\\data\\input_data_4channels_z_score\\train\\images\",\n",
    "    r\"D:\\Khedir-meriem-ESI-SBElAbes\\data\\input_data_4channels_z_score\\train\\masks\"\n",
    "    augment=True\n",
    ")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b86719",
   "metadata": {},
   "source": [
    "## üîó Ensemble Prediction: U-Net + Attention U-Net\n",
    "Combines predictions from both models to potentially improve segmentation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(x):\n",
    "    model1 = UNet3D().to(DEVICE)\n",
    "    model1.load_state_dict(torch.load(\"unet3d_best.pth\"))\n",
    "    model2 = AttentionUNet3D().to(DEVICE)\n",
    "    model2.load_state_dict(torch.load(\"att_unet3d_best.pth\"))\n",
    "    model1.eval(); model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out1 = model1(x)\n",
    "        out2 = model2(x)\n",
    "        avg_out = (out1 + out2) / 2\n",
    "        pred = torch.argmax(avg_out, dim=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_loader))\n",
    "x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "pred = ensemble_predict(x)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i*3+1); plt.imshow(x[i][0, 0].cpu(), cmap='gray'); plt.title('Input (slice 0)')\n",
    "    plt.subplot(3, 3, i*3+2); plt.imshow(y[i][0].cpu(), cmap='gray'); plt.title('Ground Truth (slice 0)')\n",
    "    plt.subplot(3, 3, i*3+3); plt.imshow(pred[i][0].cpu(), cmap='gray'); plt.title('Ensemble Pred (slice 0)')\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
