{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579c21af",
   "metadata": {},
   "source": [
    "# ğŸ§  Brain Tumor Segmentation Using 3D U-Net and Attention U-Net (PyTorch)\n",
    "### Advanced Deep Learning Pipeline for MRI Segmentation\n",
    "\n",
    "This notebook presents a complete deep learning pipeline for 3D brain tumor segmentation from MRI images using U-Net and Attention U-Net architectures.\n",
    "\n",
    "**Features:**\n",
    "- Data preprocessing and loading for multi-modal MRI\n",
    "- Custom PyTorch dataset and data loader\n",
    "- Implementation of U-Net and Attention U-Net\n",
    "- Advanced training loop with early stopping and learning rate scheduling\n",
    "- Quantitative evaluation (Dice, loss curves) and qualitative visualizations\n",
    "- Easily adaptable for research, internship, and clinical exploration\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c2d16",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d69666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('âœ… Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a955e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA available: True\n",
      "ğŸ§  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… CUDA available:\", torch.cuda.is_available())\n",
    "print(\"ğŸ§  Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44235593",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Dataset Loader: Patch-wise, Lazy Loading from `.nii.gz` (BraTS2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d72b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BraTS 2023 Patch-based 3D Dataset\n",
    "# This dataset class loads 3D patches from BraTS patient directories.\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os, random\n",
    "\n",
    "class BraTSPatch3DDataset(Dataset):\n",
    "    def __init__(self, patient_dirs, patch_depth=64, augment=False):\n",
    "        self.patch_depth = patch_depth\n",
    "        self.augment = augment\n",
    "        self.patients = []       # Preloaded patients: (image, seg)\n",
    "        self.patch_index = []    # List of (patient_idx, z_start)\n",
    "\n",
    "        for i, p in enumerate(patient_dirs):\n",
    "            pid = os.path.basename(p)\n",
    "\n",
    "            def load_modality(name):\n",
    "                path = os.path.join(p, f\"{pid}_{name}.nii.gz\")\n",
    "                return nib.load(path).get_fdata(dtype=np.float32)\n",
    "\n",
    "            # Load and normalize once\n",
    "            flair = load_modality(\"flair\")\n",
    "            t1 = load_modality(\"t1\")\n",
    "            t1ce = load_modality(\"t1ce\")\n",
    "            t2 = load_modality(\"t2\")\n",
    "            seg = load_modality(\"seg\")\n",
    "\n",
    "            image = np.stack([flair, t1, t1ce, t2], axis=0)  # Shape: (4, H, W, D)\n",
    "            for c in range(4):\n",
    "                img = image[c]\n",
    "                image[c] = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "\n",
    "            self.patients.append((image, seg))\n",
    "\n",
    "            # Create patch indices\n",
    "            depth = image.shape[-1]\n",
    "            for z in range(0, depth - patch_depth, patch_depth // 2):\n",
    "                self.patch_index.append((i, z))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx, z = self.patch_index[idx]\n",
    "        image, seg = self.patients[patient_idx]\n",
    "\n",
    "        patch_img = image[:, :, :, z:z+self.patch_depth]\n",
    "        patch_seg = seg[:, :, z:z+self.patch_depth]\n",
    "\n",
    "        patch_img = torch.tensor(patch_img.copy(), dtype=torch.float32)\n",
    "        patch_seg = torch.tensor(patch_seg.copy(), dtype=torch.long)\n",
    "\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            patch_img = torch.flip(patch_img, dims=[3])\n",
    "            patch_seg = torch.flip(patch_seg, dims=[2])\n",
    "\n",
    "        return patch_img, patch_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823361e3",
   "metadata": {},
   "source": [
    "## ğŸ”€ Dataset Split: Train & Validation\n",
    "We use an 80/20 split to train and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27317d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully\n",
      "Train patients: 1000\n",
      "Val patients: 251\n",
      "Train samples (patches): 5741\n",
      "Val samples (patches): 1425\n"
     ]
    }
   ],
   "source": [
    "# Set your BraTS2021 root directory\n",
    "root_dir = r\"C:\\Users\\amira\\Downloads\\BraTS2021_Training_Data\"\n",
    "\n",
    "# Gather patient directories\n",
    "patient_dirs = sorted([\n",
    "    os.path.join(root_dir, d)\n",
    "    for d in os.listdir(root_dir)\n",
    "    if os.path.isdir(os.path.join(root_dir, d))\n",
    "])\n",
    "random.shuffle(patient_dirs)  # Shuffle for randomness\n",
    "\n",
    "# Patient-level 80/20 split to avoid data leakage\n",
    "split_idx = int(0.8 * len(patient_dirs))\n",
    "train_patients = patient_dirs[:split_idx]\n",
    "val_patients = patient_dirs[split_idx:]\n",
    "\n",
    "# Initialize datasets\n",
    "\n",
    "class BraTSPatch3DDataset(Dataset):\n",
    "    def __init__(self, patient_dirs, patch_depth=64, augment=False):\n",
    "        self.patient_dirs = patient_dirs\n",
    "        self.patch_depth = patch_depth\n",
    "        self.augment = augment\n",
    "        self.patch_index = []  # Stores (patient_index, z_start)\n",
    "\n",
    "        # Build patch index only for tumor-containing patches\n",
    "        for i, p in enumerate(patient_dirs):\n",
    "            seg_path = os.path.join(p, os.path.basename(p) + '_seg.nii.gz')\n",
    "            seg = nib.load(seg_path).get_fdata()\n",
    "            depth = seg.shape[-1]\n",
    "\n",
    "            for z in range(0, depth - patch_depth, patch_depth // 2):\n",
    "                patch = seg[:, :, z:z+patch_depth]\n",
    "                if np.max(patch) > 0:  # âœ… Skip background-only patches\n",
    "                    self.patch_index.append((i, z))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx, z = self.patch_index[idx]\n",
    "        p = self.patient_dirs[patient_idx]\n",
    "        pid = os.path.basename(p)\n",
    "\n",
    "        def load_modality(name):\n",
    "            path = os.path.join(p, f\"{pid}_{name}.nii.gz\")\n",
    "            return nib.load(path).get_fdata(dtype=np.float32)\n",
    "\n",
    "        # Load 4 MRI modalities\n",
    "        flair = load_modality(\"flair\")\n",
    "        t1 = load_modality(\"t1\")\n",
    "        t1ce = load_modality(\"t1ce\")\n",
    "        t2 = load_modality(\"t2\")\n",
    "        seg = load_modality(\"seg\")\n",
    "\n",
    "        # Extract 3D patch: (C, H, W, D)\n",
    "        image = np.stack([flair, t1, t1ce, t2], axis=0)[:, :, :, z:z+self.patch_depth]\n",
    "        mask = seg[:, :, z:z+self.patch_depth]\n",
    "\n",
    "        # Normalize each channel independently\n",
    "        for i in range(4):\n",
    "            img = image[i]\n",
    "            image[i] = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "\n",
    "        # Convert to tensors\n",
    "        image = torch.tensor(image.copy(), dtype=torch.float32)\n",
    "        mask = torch.tensor(mask.copy(), dtype=torch.long)\n",
    "\n",
    "        # Optional horizontal flip\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            image = torch.flip(image, dims=[3])  # flip in depth\n",
    "            mask = torch.flip(mask, dims=[2])\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "train_dataset = BraTSPatch3DDataset(train_patients, patch_depth=32, augment=True)\n",
    "val_dataset = BraTSPatch3DDataset(val_patients, patch_depth=32, augment=False)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Display info\n",
    "print(\"âœ… Dataset loaded successfully\")\n",
    "print(\"Train patients:\", len(train_patients))\n",
    "print(\"Val patients:\", len(val_patients))\n",
    "print(\"Train samples (patches):\", len(train_dataset))\n",
    "print(\"Val samples (patches):\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c730cd2",
   "metadata": {},
   "source": [
    "## ğŸ§  U-Net 3D and Attention U-Net3D Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba4f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.enc1 = block(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = block(32, 64)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = block(64, 128)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, 2)\n",
    "        self.dec2 = block(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, 2)\n",
    "        self.dec1 = block(64, 32)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.dec2(torch.cat([self.up2(b), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "class AttentionBlock3D(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv3d(F_g, F_int, 1), nn.BatchNorm3d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv3d(F_l, F_int, 1), nn.BatchNorm3d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv3d(F_int, 1, 1), nn.BatchNorm3d(1), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        return x * self.psi(self.relu(self.W_g(g) + self.W_x(x)))\n",
    "\n",
    "class AttentionUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.enc1 = block(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = block(32, 64)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = block(64, 128)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, 2)\n",
    "        self.att2 = AttentionBlock3D(64, 64, 32)\n",
    "        self.dec2 = block(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, 2)\n",
    "        self.att1 = AttentionBlock3D(32, 32, 16)\n",
    "        self.dec1 = block(64, 32)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.att2(self.up2(b), e2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.att1(self.up1(d2), e1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "        return self.out_conv(d1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c48c9",
   "metadata": {},
   "source": [
    "## ğŸ§ª Training loop Function (AMP + EarlyStopping + Dice Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e013f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def dice_coeff(pred, target, eps=1e-6):\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    intersection = (pred == target).float().sum()\n",
    "    return (2. * intersection) / (pred.numel() + target.numel() + eps)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, name, epochs=100):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    best_dice = 0\n",
    "    patience = 10\n",
    "    trigger = 0\n",
    "    val_dices = []\n",
    "\n",
    "    print(f\"\\nâ³ Measuring first batch load time...\")\n",
    "    t0 = time.time()\n",
    "    _ = next(iter(train_loader))\n",
    "    print(f\"âœ… First batch loaded in {time.time() - t0:.2f} seconds.\\n\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        print(f\"\\nğŸ” Epoch {epoch}/{epochs}\")\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                # Ensure labels are within valid range\n",
    "                y = torch.clamp(y, 0, out.shape[1] - 1)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Batch progress logging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"   â†ª Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dice = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with autocast():\n",
    "                    pred = torch.argmax(model(x), dim=1)\n",
    "                dice += dice_coeff(pred.cpu(), y.cpu())\n",
    "\n",
    "        avg_dice = dice / len(val_loader)\n",
    "        val_dices.append(avg_dice.item())\n",
    "        print(f\"âœ… Epoch {epoch} complete | Loss: {total_loss:.4f} | Val Dice: {avg_dice:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "            print(f\"ğŸ’¾ Best model saved with Dice: {best_dice:.4f}\")\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            print(f\"âš ï¸ No improvement. Patience: {trigger}/{patience}\")\n",
    "            if trigger >= patience:\n",
    "                print(\"â¹ï¸ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Plot Dice over epochs\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(val_dices, label=\"Validation Dice\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Dice Score\")\n",
    "    plt.title(f\"Dice Curve for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce991be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max label value in dataset: 4\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _, y in train_loader:\n",
    "    labels.append(y.max().item())\n",
    "    break\n",
    "\n",
    "print(\"Max label value in dataset:\", max(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a024cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Measuring first batch load time...\n",
      "âœ… First batch loaded in 0.34 seconds.\n",
      "\n",
      "\n",
      "ğŸ” Epoch 1/100\n",
      "   â†ª Batch 0/5741 | Loss: 1.2150\n",
      "   â†ª Batch 100/5741 | Loss: 0.7321\n",
      "   â†ª Batch 200/5741 | Loss: 0.6350\n",
      "   â†ª Batch 300/5741 | Loss: 0.4842\n",
      "   â†ª Batch 400/5741 | Loss: 0.3879\n",
      "   â†ª Batch 500/5741 | Loss: 0.3242\n",
      "   â†ª Batch 600/5741 | Loss: 0.2751\n",
      "   â†ª Batch 700/5741 | Loss: 0.2488\n",
      "   â†ª Batch 800/5741 | Loss: 0.1906\n",
      "   â†ª Batch 900/5741 | Loss: 0.1670\n",
      "   â†ª Batch 1000/5741 | Loss: 0.1460\n",
      "   â†ª Batch 1100/5741 | Loss: 0.1246\n",
      "   â†ª Batch 1200/5741 | Loss: 0.1154\n",
      "   â†ª Batch 1300/5741 | Loss: 0.1065\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0789\n",
      "   â†ª Batch 1500/5741 | Loss: 0.1102\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0640\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0641\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0525\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0449\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0430\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0661\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0327\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0291\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0298\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0351\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0294\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0262\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0304\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0214\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0259\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0346\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0154\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0130\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0236\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0214\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0186\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0292\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0179\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0086\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0214\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0117\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0163\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0186\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0303\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0191\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0441\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0050\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0249\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0117\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0109\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0132\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0157\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0350\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0440\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0056\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0334\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0164\n",
      "âœ… Epoch 1 complete | Loss: 604.4327 | Val Dice: 0.9865\n",
      "ğŸ’¾ Best model saved with Dice: 0.9865\n",
      "\n",
      "ğŸ” Epoch 2/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0241\n",
      "   â†ª Batch 100/5741 | Loss: 0.0041\n",
      "   â†ª Batch 200/5741 | Loss: 0.0110\n",
      "   â†ª Batch 300/5741 | Loss: 0.0295\n",
      "   â†ª Batch 400/5741 | Loss: 0.0145\n",
      "   â†ª Batch 500/5741 | Loss: 0.0054\n",
      "   â†ª Batch 600/5741 | Loss: 0.0227\n",
      "   â†ª Batch 700/5741 | Loss: 0.0028\n",
      "   â†ª Batch 800/5741 | Loss: 0.0101\n",
      "   â†ª Batch 900/5741 | Loss: 0.0086\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0043\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0263\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0243\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0061\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0132\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0043\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0056\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0022\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0125\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0243\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0020\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0034\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0559\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0352\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0374\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0179\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0220\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0052\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0060\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0461\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0253\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0185\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0063\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0100\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0186\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0015\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0016\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0017\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0125\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0661\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0134\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0144\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0128\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0061\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0025\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0454\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0169\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0237\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0021\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0022\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0314\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0052\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0045\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0156\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0059\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0174\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0077\n",
      "âœ… Epoch 2 complete | Loss: 89.9143 | Val Dice: 0.9832\n",
      "âš ï¸ No improvement. Patience: 1/10\n",
      "\n",
      "ğŸ” Epoch 3/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0061\n",
      "   â†ª Batch 100/5741 | Loss: 0.0117\n",
      "   â†ª Batch 200/5741 | Loss: 0.0121\n",
      "   â†ª Batch 300/5741 | Loss: 0.0056\n",
      "   â†ª Batch 400/5741 | Loss: 0.0009\n",
      "   â†ª Batch 500/5741 | Loss: 0.0083\n",
      "   â†ª Batch 600/5741 | Loss: 0.0099\n",
      "   â†ª Batch 700/5741 | Loss: 0.0031\n",
      "   â†ª Batch 800/5741 | Loss: 0.0041\n",
      "   â†ª Batch 900/5741 | Loss: 0.0341\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0101\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0185\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0006\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0010\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0115\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0129\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0532\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0420\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0058\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0003\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0098\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0412\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0030\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0110\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0019\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0105\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0149\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0158\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0047\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0066\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0369\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0086\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0094\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0133\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0021\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0236\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0396\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0084\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0025\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0109\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0068\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0030\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0136\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0679\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0207\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0072\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0184\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0254\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0034\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0121\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0015\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0088\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0097\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0033\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0069\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0040\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0180\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0233\n",
      "âœ… Epoch 3 complete | Loss: 76.4825 | Val Dice: 0.9832\n",
      "âš ï¸ No improvement. Patience: 2/10\n",
      "\n",
      "ğŸ” Epoch 4/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0070\n",
      "   â†ª Batch 100/5741 | Loss: 0.0145\n",
      "   â†ª Batch 200/5741 | Loss: 0.0125\n",
      "   â†ª Batch 300/5741 | Loss: 0.0159\n",
      "   â†ª Batch 400/5741 | Loss: 0.0183\n",
      "   â†ª Batch 500/5741 | Loss: 0.0086\n",
      "   â†ª Batch 600/5741 | Loss: 0.0010\n",
      "   â†ª Batch 700/5741 | Loss: 0.0048\n",
      "   â†ª Batch 800/5741 | Loss: 0.0231\n",
      "   â†ª Batch 900/5741 | Loss: 0.0174\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0315\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0181\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0107\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0244\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0084\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0048\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0674\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0017\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0179\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0373\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0066\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0222\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0031\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0262\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0011\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0086\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0208\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0043\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0045\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0196\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0422\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0038\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0041\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0266\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0114\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0234\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0069\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0039\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0029\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0004\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0071\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0051\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0062\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0048\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0063\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0116\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0083\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0146\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0128\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0315\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0106\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0025\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0168\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0183\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0114\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0120\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0011\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0115\n",
      "âœ… Epoch 4 complete | Loss: 68.7147 | Val Dice: 0.9839\n",
      "âš ï¸ No improvement. Patience: 3/10\n",
      "\n",
      "ğŸ” Epoch 5/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0045\n",
      "   â†ª Batch 100/5741 | Loss: 0.0068\n",
      "   â†ª Batch 200/5741 | Loss: 0.0051\n",
      "   â†ª Batch 300/5741 | Loss: 0.0093\n",
      "   â†ª Batch 400/5741 | Loss: 0.0021\n",
      "   â†ª Batch 500/5741 | Loss: 0.0002\n",
      "   â†ª Batch 600/5741 | Loss: 0.0003\n",
      "   â†ª Batch 700/5741 | Loss: 0.0070\n",
      "   â†ª Batch 800/5741 | Loss: 0.0024\n",
      "   â†ª Batch 900/5741 | Loss: 0.0239\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0069\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0015\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0210\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0282\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0084\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0535\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0053\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0013\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0048\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0008\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0045\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0202\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0101\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0036\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0052\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0066\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0061\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0448\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0077\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0108\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0110\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0094\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0067\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0189\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0205\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0043\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0016\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0026\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0308\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0059\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0031\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0081\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0171\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0355\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0187\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0024\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0121\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0043\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0193\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0182\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0018\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0018\n",
      "âœ… Epoch 5 complete | Loss: 63.6082 | Val Dice: 0.9856\n",
      "âš ï¸ No improvement. Patience: 4/10\n",
      "\n",
      "ğŸ” Epoch 6/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0084\n",
      "   â†ª Batch 100/5741 | Loss: 0.0135\n",
      "   â†ª Batch 200/5741 | Loss: 0.0213\n",
      "   â†ª Batch 300/5741 | Loss: 0.0006\n",
      "   â†ª Batch 400/5741 | Loss: 0.0062\n",
      "   â†ª Batch 500/5741 | Loss: 0.0082\n",
      "   â†ª Batch 600/5741 | Loss: 0.0080\n",
      "   â†ª Batch 700/5741 | Loss: 0.0039\n",
      "   â†ª Batch 800/5741 | Loss: 0.0012\n",
      "   â†ª Batch 900/5741 | Loss: 0.0016\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0153\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0031\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0205\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0355\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0124\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0008\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0021\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0153\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0291\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0110\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0219\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0215\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0061\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0019\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0056\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0108\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0013\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0098\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0417\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0064\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0352\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0076\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0051\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0055\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0046\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0060\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0022\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0058\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0085\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0575\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0060\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0014\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0567\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0201\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0157\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0018\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0315\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0016\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0033\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0185\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0139\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0235\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0115\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0039\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0025\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0027\n",
      "âœ… Epoch 6 complete | Loss: 61.7080 | Val Dice: 0.9804\n",
      "âš ï¸ No improvement. Patience: 5/10\n",
      "\n",
      "ğŸ” Epoch 7/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0076\n",
      "   â†ª Batch 100/5741 | Loss: 0.0002\n",
      "   â†ª Batch 200/5741 | Loss: 0.0028\n",
      "   â†ª Batch 300/5741 | Loss: 0.0080\n",
      "   â†ª Batch 400/5741 | Loss: 0.0048\n",
      "   â†ª Batch 500/5741 | Loss: 0.0101\n",
      "   â†ª Batch 600/5741 | Loss: 0.0075\n",
      "   â†ª Batch 700/5741 | Loss: 0.0002\n",
      "   â†ª Batch 800/5741 | Loss: 0.0006\n",
      "   â†ª Batch 900/5741 | Loss: 0.0092\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0259\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0044\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0055\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0063\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0029\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0025\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0258\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0111\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0090\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0009\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0084\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0051\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0088\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0030\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0022\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0036\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0265\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0589\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0260\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0027\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0079\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0080\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0368\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0077\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0054\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0158\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0006\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0043\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0019\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0040\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0064\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0035\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0010\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0027\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0053\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0372\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0109\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0027\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0195\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0021\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0092\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0172\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0014\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0283\n",
      "âœ… Epoch 7 complete | Loss: 58.1061 | Val Dice: 0.9843\n",
      "âš ï¸ No improvement. Patience: 6/10\n",
      "\n",
      "ğŸ” Epoch 8/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0171\n",
      "   â†ª Batch 100/5741 | Loss: 0.0152\n",
      "   â†ª Batch 200/5741 | Loss: 0.0053\n",
      "   â†ª Batch 300/5741 | Loss: 0.0103\n",
      "   â†ª Batch 400/5741 | Loss: 0.0025\n",
      "   â†ª Batch 500/5741 | Loss: 0.0241\n",
      "   â†ª Batch 600/5741 | Loss: 0.0292\n",
      "   â†ª Batch 700/5741 | Loss: 0.0024\n",
      "   â†ª Batch 800/5741 | Loss: 0.0030\n",
      "   â†ª Batch 900/5741 | Loss: 0.0115\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0260\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0093\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0096\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0013\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0057\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0089\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0075\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0092\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0137\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0027\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0023\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0227\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0180\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0031\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0165\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0085\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0330\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0151\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0045\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0008\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0037\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0192\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0076\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0022\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0396\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0004\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0076\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0289\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0012\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0128\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0523\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0760\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0077\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0025\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0032\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0031\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0120\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0106\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0068\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0048\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0113\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0010\n",
      "âœ… Epoch 8 complete | Loss: 56.3270 | Val Dice: 0.9853\n",
      "âš ï¸ No improvement. Patience: 7/10\n",
      "\n",
      "ğŸ” Epoch 9/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0037\n",
      "   â†ª Batch 100/5741 | Loss: 0.0128\n",
      "   â†ª Batch 200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 300/5741 | Loss: 0.0065\n",
      "   â†ª Batch 400/5741 | Loss: 0.0213\n",
      "   â†ª Batch 500/5741 | Loss: 0.0242\n",
      "   â†ª Batch 600/5741 | Loss: 0.0148\n",
      "   â†ª Batch 700/5741 | Loss: 0.0166\n",
      "   â†ª Batch 800/5741 | Loss: 0.0187\n",
      "   â†ª Batch 900/5741 | Loss: 0.0021\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0273\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0036\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0055\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0017\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0125\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0052\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0035\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0248\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0056\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0322\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0018\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0022\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0122\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0106\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0549\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0011\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0097\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0156\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0161\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0081\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0020\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0049\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0148\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0066\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0019\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0283\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0108\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0235\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0008\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0041\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0019\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0003\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0099\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0131\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0035\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0050\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0348\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0018\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0182\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0096\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0212\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0089\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0056\n",
      "âœ… Epoch 9 complete | Loss: 54.6467 | Val Dice: 0.9871\n",
      "ğŸ’¾ Best model saved with Dice: 0.9871\n",
      "\n",
      "ğŸ” Epoch 10/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0112\n",
      "   â†ª Batch 100/5741 | Loss: 0.0402\n",
      "   â†ª Batch 200/5741 | Loss: 0.0065\n",
      "   â†ª Batch 300/5741 | Loss: 0.0006\n",
      "   â†ª Batch 400/5741 | Loss: 0.0015\n",
      "   â†ª Batch 500/5741 | Loss: 0.0010\n",
      "   â†ª Batch 600/5741 | Loss: 0.0155\n",
      "   â†ª Batch 700/5741 | Loss: 0.0251\n",
      "   â†ª Batch 800/5741 | Loss: 0.0123\n",
      "   â†ª Batch 900/5741 | Loss: 0.0028\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0105\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0144\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0140\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0118\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0229\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0059\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0125\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0042\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0162\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0043\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0073\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0016\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0034\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0183\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0074\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0587\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0014\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0050\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0003\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0029\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0021\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0042\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0152\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0084\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0322\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0030\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0084\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0022\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0183\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0019\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0012\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0165\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0030\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0041\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0051\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0234\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0050\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0033\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0040\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0470\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0098\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0020\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0072\n",
      "âœ… Epoch 10 complete | Loss: 53.8352 | Val Dice: 0.9775\n",
      "âš ï¸ No improvement. Patience: 1/10\n",
      "\n",
      "ğŸ” Epoch 11/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0127\n",
      "   â†ª Batch 100/5741 | Loss: 0.0156\n",
      "   â†ª Batch 200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 300/5741 | Loss: 0.0024\n",
      "   â†ª Batch 400/5741 | Loss: 0.0046\n",
      "   â†ª Batch 500/5741 | Loss: 0.0044\n",
      "   â†ª Batch 600/5741 | Loss: 0.0051\n",
      "   â†ª Batch 700/5741 | Loss: 0.0058\n",
      "   â†ª Batch 800/5741 | Loss: 0.0040\n",
      "   â†ª Batch 900/5741 | Loss: 0.0019\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0012\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0138\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0138\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0085\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0405\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0071\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0135\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0067\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0078\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0137\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0479\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0192\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0025\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0046\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0079\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0070\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0051\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0010\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0094\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0043\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0362\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0023\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0093\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0013\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0059\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0005\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0040\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0088\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0187\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0249\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0074\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0151\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0148\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0037\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0062\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0011\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0080\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0126\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0076\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0066\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0037\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0015\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0133\n",
      "âœ… Epoch 11 complete | Loss: 51.9028 | Val Dice: 0.9852\n",
      "âš ï¸ No improvement. Patience: 2/10\n",
      "\n",
      "ğŸ” Epoch 12/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0082\n",
      "   â†ª Batch 100/5741 | Loss: 0.0055\n",
      "   â†ª Batch 200/5741 | Loss: 0.0001\n",
      "   â†ª Batch 300/5741 | Loss: 0.0017\n",
      "   â†ª Batch 400/5741 | Loss: 0.0132\n",
      "   â†ª Batch 500/5741 | Loss: 0.0137\n",
      "   â†ª Batch 600/5741 | Loss: 0.0111\n",
      "   â†ª Batch 700/5741 | Loss: 0.0030\n",
      "   â†ª Batch 800/5741 | Loss: 0.0104\n",
      "   â†ª Batch 900/5741 | Loss: 0.0067\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0080\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0014\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0202\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0303\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0046\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0056\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0031\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0063\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0090\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0025\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0043\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0079\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0067\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0160\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0003\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0006\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0053\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0096\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0185\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0003\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0054\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0009\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0031\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0020\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0148\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0060\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0081\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0066\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0154\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0030\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0013\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0127\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0087\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0001\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0015\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0024\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0013\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0070\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0331\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0071\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0018\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0043\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0078\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0059\n",
      "âœ… Epoch 12 complete | Loss: 51.0868 | Val Dice: 0.9845\n",
      "âš ï¸ No improvement. Patience: 3/10\n",
      "\n",
      "ğŸ” Epoch 13/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0072\n",
      "   â†ª Batch 100/5741 | Loss: 0.0029\n",
      "   â†ª Batch 200/5741 | Loss: 0.0101\n",
      "   â†ª Batch 300/5741 | Loss: 0.0018\n",
      "   â†ª Batch 400/5741 | Loss: 0.0072\n",
      "   â†ª Batch 500/5741 | Loss: 0.0044\n",
      "   â†ª Batch 600/5741 | Loss: 0.0033\n",
      "   â†ª Batch 700/5741 | Loss: 0.0208\n",
      "   â†ª Batch 800/5741 | Loss: 0.0012\n",
      "   â†ª Batch 900/5741 | Loss: 0.0120\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0089\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0041\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0050\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0095\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0070\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0107\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0102\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0120\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0006\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0085\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0050\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0004\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0024\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0100\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0053\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0048\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0053\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0050\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0070\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0069\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0103\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0005\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0026\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0118\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0043\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0052\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0004\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0118\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0131\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0131\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0136\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0303\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0049\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0018\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0433\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0043\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0007\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0053\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0011\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0030\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0142\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0029\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0084\n",
      "âœ… Epoch 13 complete | Loss: 50.1445 | Val Dice: 0.9859\n",
      "âš ï¸ No improvement. Patience: 4/10\n",
      "\n",
      "ğŸ” Epoch 14/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0043\n",
      "   â†ª Batch 100/5741 | Loss: 0.0095\n",
      "   â†ª Batch 200/5741 | Loss: 0.0025\n",
      "   â†ª Batch 300/5741 | Loss: 0.0118\n",
      "   â†ª Batch 400/5741 | Loss: 0.0037\n",
      "   â†ª Batch 500/5741 | Loss: 0.0081\n",
      "   â†ª Batch 600/5741 | Loss: 0.0200\n",
      "   â†ª Batch 700/5741 | Loss: 0.0078\n",
      "   â†ª Batch 800/5741 | Loss: 0.0042\n",
      "   â†ª Batch 900/5741 | Loss: 0.0118\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0036\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0108\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0001\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0001\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0076\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0135\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0064\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0000\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0160\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0082\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0048\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0061\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0010\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0081\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0019\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0034\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0275\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0035\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0083\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0039\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0008\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0041\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0121\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0242\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0048\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0146\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0051\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0015\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0066\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0039\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0015\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0135\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0086\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0006\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0048\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0062\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0178\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0022\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0085\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0036\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0179\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0481\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0088\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0042\n",
      "âœ… Epoch 14 complete | Loss: 48.4997 | Val Dice: 0.9889\n",
      "ğŸ’¾ Best model saved with Dice: 0.9889\n",
      "\n",
      "ğŸ” Epoch 15/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0056\n",
      "   â†ª Batch 100/5741 | Loss: 0.0124\n",
      "   â†ª Batch 200/5741 | Loss: 0.0063\n",
      "   â†ª Batch 300/5741 | Loss: 0.0056\n",
      "   â†ª Batch 400/5741 | Loss: 0.0084\n",
      "   â†ª Batch 500/5741 | Loss: 0.0011\n",
      "   â†ª Batch 600/5741 | Loss: 0.0010\n",
      "   â†ª Batch 700/5741 | Loss: 0.0206\n",
      "   â†ª Batch 800/5741 | Loss: 0.0043\n",
      "   â†ª Batch 900/5741 | Loss: 0.0441\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0046\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0115\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0032\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0092\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0106\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0245\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0076\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0061\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0040\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0082\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0016\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0146\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0040\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0045\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0039\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0082\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0061\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0093\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0076\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0103\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0204\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0045\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0024\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0005\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0116\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0081\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0031\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0161\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0168\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0087\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0050\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0207\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0048\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0052\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0062\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0142\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0060\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0026\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0088\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0269\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0040\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0041\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0093\n",
      "âœ… Epoch 15 complete | Loss: 48.0835 | Val Dice: 0.9842\n",
      "âš ï¸ No improvement. Patience: 1/10\n",
      "\n",
      "ğŸ” Epoch 16/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0194\n",
      "   â†ª Batch 100/5741 | Loss: 0.0076\n",
      "   â†ª Batch 200/5741 | Loss: 0.0423\n",
      "   â†ª Batch 300/5741 | Loss: 0.0093\n",
      "   â†ª Batch 400/5741 | Loss: 0.0070\n",
      "   â†ª Batch 500/5741 | Loss: 0.0008\n",
      "   â†ª Batch 600/5741 | Loss: 0.0043\n",
      "   â†ª Batch 700/5741 | Loss: 0.0007\n",
      "   â†ª Batch 800/5741 | Loss: 0.0049\n",
      "   â†ª Batch 900/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0054\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0010\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0307\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0119\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0026\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0187\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0022\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0219\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0106\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0443\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0007\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0029\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0097\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0066\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0050\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0012\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0015\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0080\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0026\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0028\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0234\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0048\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0098\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0092\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0041\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0126\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0105\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0137\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0032\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0253\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0047\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0052\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0508\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0003\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0040\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0003\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0034\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0004\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0062\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0071\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0040\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0011\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0021\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0071\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0005\n",
      "âœ… Epoch 16 complete | Loss: 47.4199 | Val Dice: 0.9872\n",
      "âš ï¸ No improvement. Patience: 2/10\n",
      "\n",
      "ğŸ” Epoch 17/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0093\n",
      "   â†ª Batch 100/5741 | Loss: 0.0100\n",
      "   â†ª Batch 200/5741 | Loss: 0.0049\n",
      "   â†ª Batch 300/5741 | Loss: 0.0023\n",
      "   â†ª Batch 400/5741 | Loss: 0.0025\n",
      "   â†ª Batch 500/5741 | Loss: 0.0336\n",
      "   â†ª Batch 600/5741 | Loss: 0.0068\n",
      "   â†ª Batch 700/5741 | Loss: 0.0003\n",
      "   â†ª Batch 800/5741 | Loss: 0.0086\n",
      "   â†ª Batch 900/5741 | Loss: 0.0032\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0036\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0030\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0027\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0032\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0103\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0019\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0019\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0585\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0004\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0013\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0224\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0056\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0029\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0013\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0205\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0021\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0160\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0092\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0061\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0034\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0057\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0249\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0177\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0273\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0585\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0012\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0031\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0028\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0552\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0326\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0034\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0027\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0059\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0212\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0108\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0082\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0021\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0018\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0090\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0023\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0121\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0058\n",
      "âœ… Epoch 17 complete | Loss: 46.5843 | Val Dice: 0.9865\n",
      "âš ï¸ No improvement. Patience: 3/10\n",
      "\n",
      "ğŸ” Epoch 18/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0001\n",
      "   â†ª Batch 100/5741 | Loss: 0.0011\n",
      "   â†ª Batch 200/5741 | Loss: 0.0027\n",
      "   â†ª Batch 300/5741 | Loss: 0.0054\n",
      "   â†ª Batch 400/5741 | Loss: 0.0009\n",
      "   â†ª Batch 500/5741 | Loss: 0.0038\n",
      "   â†ª Batch 600/5741 | Loss: 0.0011\n",
      "   â†ª Batch 700/5741 | Loss: 0.0002\n",
      "   â†ª Batch 800/5741 | Loss: 0.0058\n",
      "   â†ª Batch 900/5741 | Loss: 0.0043\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0248\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0022\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0056\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0287\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0037\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0016\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0025\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0011\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0223\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0012\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0025\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0005\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0006\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0023\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0366\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0208\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0065\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0391\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0094\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0143\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0085\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0041\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0308\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0026\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0028\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0075\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0031\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0029\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0003\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0001\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0029\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0005\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0134\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0078\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0039\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0076\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0013\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0073\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0010\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0430\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0050\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0023\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0057\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0027\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0009\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0023\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0057\n",
      "âœ… Epoch 18 complete | Loss: 46.2668 | Val Dice: 0.9851\n",
      "âš ï¸ No improvement. Patience: 4/10\n",
      "\n",
      "ğŸ” Epoch 19/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0078\n",
      "   â†ª Batch 100/5741 | Loss: 0.0065\n",
      "   â†ª Batch 200/5741 | Loss: 0.0012\n",
      "   â†ª Batch 300/5741 | Loss: 0.0019\n",
      "   â†ª Batch 400/5741 | Loss: 0.0072\n",
      "   â†ª Batch 500/5741 | Loss: 0.0189\n",
      "   â†ª Batch 600/5741 | Loss: 0.0044\n",
      "   â†ª Batch 700/5741 | Loss: 0.0003\n",
      "   â†ª Batch 800/5741 | Loss: 0.0009\n",
      "   â†ª Batch 900/5741 | Loss: 0.0078\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0142\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0036\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0027\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0072\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0118\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0050\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0011\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0041\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0020\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0024\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0056\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0072\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0057\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0018\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0026\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0199\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0085\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0087\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0108\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0071\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0136\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0014\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0133\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0047\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0061\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0032\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0058\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0070\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0002\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0229\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0004\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0244\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0067\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0034\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0071\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0096\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0004\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0056\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0029\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0118\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0031\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0032\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0016\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0189\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0071\n",
      "âœ… Epoch 19 complete | Loss: 45.1954 | Val Dice: 0.9839\n",
      "âš ï¸ No improvement. Patience: 5/10\n",
      "\n",
      "ğŸ” Epoch 20/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0054\n",
      "   â†ª Batch 100/5741 | Loss: 0.0038\n",
      "   â†ª Batch 200/5741 | Loss: 0.0865\n",
      "   â†ª Batch 300/5741 | Loss: 0.0044\n",
      "   â†ª Batch 400/5741 | Loss: 0.0059\n",
      "   â†ª Batch 500/5741 | Loss: 0.0011\n",
      "   â†ª Batch 600/5741 | Loss: 0.0020\n",
      "   â†ª Batch 700/5741 | Loss: 0.0011\n",
      "   â†ª Batch 800/5741 | Loss: 0.0197\n",
      "   â†ª Batch 900/5741 | Loss: 0.0002\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0221\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0073\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0080\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0073\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0147\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0125\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0026\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0116\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0051\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0021\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0009\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0047\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0053\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0045\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0024\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0124\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0002\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0132\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0165\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0002\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0040\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0065\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0015\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0050\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0123\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0028\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0235\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0021\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0010\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0007\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0050\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0095\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0163\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0035\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0106\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0059\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0095\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0065\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0092\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0005\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0077\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0034\n",
      "âœ… Epoch 20 complete | Loss: 44.8109 | Val Dice: 0.9865\n",
      "âš ï¸ No improvement. Patience: 6/10\n",
      "\n",
      "ğŸ” Epoch 21/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0039\n",
      "   â†ª Batch 100/5741 | Loss: 0.0125\n",
      "   â†ª Batch 200/5741 | Loss: 0.0087\n",
      "   â†ª Batch 300/5741 | Loss: 0.0025\n",
      "   â†ª Batch 400/5741 | Loss: 0.0076\n",
      "   â†ª Batch 500/5741 | Loss: 0.0022\n",
      "   â†ª Batch 600/5741 | Loss: 0.0074\n",
      "   â†ª Batch 700/5741 | Loss: 0.0057\n",
      "   â†ª Batch 800/5741 | Loss: 0.0012\n",
      "   â†ª Batch 900/5741 | Loss: 0.0006\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0004\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0151\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0077\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0037\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0055\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0136\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0089\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0062\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0252\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0016\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0008\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0092\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0042\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0066\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0033\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0037\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0032\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0009\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0023\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0034\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0010\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0010\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0117\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0009\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0027\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0399\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0052\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0137\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0001\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0086\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0084\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0019\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0109\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0080\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0425\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0071\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0024\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0078\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0163\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0089\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0103\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0129\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0030\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0023\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0060\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0076\n",
      "âœ… Epoch 21 complete | Loss: 44.4175 | Val Dice: 0.9877\n",
      "âš ï¸ No improvement. Patience: 7/10\n",
      "\n",
      "ğŸ” Epoch 22/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0480\n",
      "   â†ª Batch 100/5741 | Loss: 0.0022\n",
      "   â†ª Batch 200/5741 | Loss: 0.0086\n",
      "   â†ª Batch 300/5741 | Loss: 0.0041\n",
      "   â†ª Batch 400/5741 | Loss: 0.0052\n",
      "   â†ª Batch 500/5741 | Loss: 0.0091\n",
      "   â†ª Batch 600/5741 | Loss: 0.0104\n",
      "   â†ª Batch 700/5741 | Loss: 0.0007\n",
      "   â†ª Batch 800/5741 | Loss: 0.0099\n",
      "   â†ª Batch 900/5741 | Loss: 0.0028\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0012\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0015\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0063\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0058\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0072\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0128\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0014\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0037\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0046\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0020\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0027\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0002\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0112\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0049\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0088\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0021\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0159\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0002\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0092\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0056\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0017\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0020\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0089\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0302\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0069\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0512\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0031\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0082\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0163\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0056\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0084\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0008\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0014\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0025\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0031\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0004\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0076\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0041\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0187\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0099\n",
      "âœ… Epoch 22 complete | Loss: 43.5542 | Val Dice: 0.9880\n",
      "âš ï¸ No improvement. Patience: 8/10\n",
      "\n",
      "ğŸ” Epoch 23/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0006\n",
      "   â†ª Batch 100/5741 | Loss: 0.0031\n",
      "   â†ª Batch 200/5741 | Loss: 0.0004\n",
      "   â†ª Batch 300/5741 | Loss: 0.0061\n",
      "   â†ª Batch 400/5741 | Loss: 0.0051\n",
      "   â†ª Batch 500/5741 | Loss: 0.0014\n",
      "   â†ª Batch 600/5741 | Loss: 0.0017\n",
      "   â†ª Batch 700/5741 | Loss: 0.0074\n",
      "   â†ª Batch 800/5741 | Loss: 0.0103\n",
      "   â†ª Batch 900/5741 | Loss: 0.0382\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0220\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0039\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0151\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0080\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0022\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0040\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0053\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0063\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0002\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0032\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0151\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0007\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0107\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0046\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0099\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0053\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0018\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0012\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0002\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0172\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0059\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0268\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0053\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0031\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0492\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0091\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0057\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0015\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0045\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0029\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0008\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0039\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0018\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0067\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0019\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0050\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0016\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0013\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0035\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0031\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0073\n",
      "âœ… Epoch 23 complete | Loss: 42.9294 | Val Dice: 0.9866\n",
      "âš ï¸ No improvement. Patience: 9/10\n",
      "\n",
      "ğŸ” Epoch 24/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0061\n",
      "   â†ª Batch 100/5741 | Loss: 0.0186\n",
      "   â†ª Batch 200/5741 | Loss: 0.0055\n",
      "   â†ª Batch 300/5741 | Loss: 0.0070\n",
      "   â†ª Batch 400/5741 | Loss: 0.0025\n",
      "   â†ª Batch 500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 600/5741 | Loss: 0.0083\n",
      "   â†ª Batch 700/5741 | Loss: 0.0024\n",
      "   â†ª Batch 800/5741 | Loss: 0.0122\n",
      "   â†ª Batch 900/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0020\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0065\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0104\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0133\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0005\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0052\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0046\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0005\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0102\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0086\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0035\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0010\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0127\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0240\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0038\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0250\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0094\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0036\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0087\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0004\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0206\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0087\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0107\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0129\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0038\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0009\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0113\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0029\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0104\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0009\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0161\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0034\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0024\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0033\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0069\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0025\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0003\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0062\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0003\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0019\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0042\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0088\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0105\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0209\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0043\n",
      "âœ… Epoch 24 complete | Loss: 42.4260 | Val Dice: 0.9875\n",
      "âš ï¸ No improvement. Patience: 10/10\n",
      "â¹ï¸ Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet = UNet3D()\n",
    "train_model(unet, train_loader, val_loader, name=\"unet3d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d714e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from medpy.metric.binary import hd95\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def evaluate_model(model, val_loader, num_classes=4):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    hd95s = [[] for _ in range(num_classes)]\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"none\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            preds = model(x)\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            # MONAI Dice expects shape (B, C, ...)\n",
    "            y_onehot = torch.nn.functional.one_hot(y, num_classes=num_classes).permute(0, 4, 1, 2, 3)\n",
    "            p_onehot = torch.nn.functional.one_hot(preds, num_classes=num_classes).permute(0, 4, 1, 2, 3)\n",
    "\n",
    "            dice = dice_metric(p_onehot.float(), y_onehot.float())\n",
    "            dices.append(dice.cpu().numpy())\n",
    "\n",
    "            y_np = y.cpu().numpy()\n",
    "            pred_np = preds.cpu().numpy()\n",
    "\n",
    "            # Hausdorff (per class, per sample)\n",
    "            for b in range(pred_np.shape[0]):\n",
    "                for cls in range(num_classes):\n",
    "                    gt_bin = (y_np[b] == cls).astype(np.uint8)\n",
    "                    pred_bin = (pred_np[b] == cls).astype(np.uint8)\n",
    "                    if np.any(gt_bin) and np.any(pred_bin):\n",
    "                        hd = hd95(pred_bin, gt_bin)\n",
    "                        hd95s[cls].append(hd)\n",
    "\n",
    "    # Final Scores\n",
    "    dices = np.vstack(dices)\n",
    "    mean_dice_per_class = np.nanmean(dices, axis=0)\n",
    "\n",
    "    mean_hd95_per_class = [np.mean(hd) if len(hd) > 0 else np.nan for hd in hd95s]\n",
    "\n",
    "    print(\"\\nğŸ“Š **Per-Class Dice Scores:**\")\n",
    "    for i, d in enumerate(mean_dice_per_class):\n",
    "        print(f\"  Class {i}: Dice = {d:.4f}\")\n",
    "\n",
    "    print(\"\\nğŸ“ **Per-Class Hausdorff 95 Distances (lower is better):**\")\n",
    "    for i, h in enumerate(mean_hd95_per_class):\n",
    "        print(f\"  Class {i}: HD95 = {h:.2f} voxels\")\n",
    "\n",
    "    return mean_dice_per_class, mean_hd95_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be424d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUNet3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet3d_best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Advanced Brain Tumor Segmentation and Augmented Reality Integration\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Advanced Brain Tumor Segmentation and Augmented Reality Integration\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Advanced Brain Tumor Segmentation and Augmented Reality Integration\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Advanced Brain Tumor Segmentation and Augmented Reality Integration\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Advanced Brain Tumor Segmentation and Augmented Reality Integration\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = UNet3D(out_channels=3).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"unet3d_best.pth\"))\n",
    "model.eval()\n",
    "evaluate_model(model, val_loader, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5916089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Measuring first batch load time...\n",
      "âœ… First batch loaded in 0.31 seconds.\n",
      "\n",
      "\n",
      "ğŸ” Epoch 1/100\n",
      "   â†ª Batch 0/5741 | Loss: 1.0701\n",
      "   â†ª Batch 100/5741 | Loss: 0.5815\n",
      "   â†ª Batch 200/5741 | Loss: 0.4653\n",
      "   â†ª Batch 300/5741 | Loss: 0.3809\n",
      "   â†ª Batch 400/5741 | Loss: 0.3556\n",
      "   â†ª Batch 500/5741 | Loss: 0.3160\n",
      "   â†ª Batch 600/5741 | Loss: 0.2126\n",
      "   â†ª Batch 700/5741 | Loss: 0.1837\n",
      "   â†ª Batch 800/5741 | Loss: 0.1390\n",
      "   â†ª Batch 900/5741 | Loss: 0.1282\n",
      "   â†ª Batch 1000/5741 | Loss: 0.1277\n",
      "   â†ª Batch 1100/5741 | Loss: 0.1010\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0795\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0896\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0651\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0561\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0486\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0408\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0377\n",
      "   â†ª Batch 1900/5741 | Loss: 0.1360\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0358\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0323\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0272\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0843\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0488\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0439\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0328\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0194\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0260\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0208\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0516\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0296\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0180\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0198\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0528\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0523\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0280\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0307\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0446\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0155\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0374\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0161\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0098\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0221\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0108\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0115\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0048\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0155\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0173\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0539\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0119\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0085\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0066\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0057\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0080\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0054\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0072\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0083\n",
      "âœ… Epoch 1 complete | Loss: 500.9284 | Val Dice: 0.9874\n",
      "ğŸ’¾ Best model saved with Dice: 0.9874\n",
      "\n",
      "ğŸ” Epoch 2/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0829\n",
      "   â†ª Batch 100/5741 | Loss: 0.0354\n",
      "   â†ª Batch 200/5741 | Loss: 0.0701\n",
      "   â†ª Batch 300/5741 | Loss: 0.0080\n",
      "   â†ª Batch 400/5741 | Loss: 0.0294\n",
      "   â†ª Batch 500/5741 | Loss: 0.0225\n",
      "   â†ª Batch 600/5741 | Loss: 0.0491\n",
      "   â†ª Batch 700/5741 | Loss: 0.0152\n",
      "   â†ª Batch 800/5741 | Loss: 0.0929\n",
      "   â†ª Batch 900/5741 | Loss: 0.0380\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0092\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0055\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0049\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0056\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0088\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0017\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0148\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0056\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0077\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0272\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0063\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0633\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0131\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0065\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0111\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0016\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0106\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0033\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0020\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0030\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0383\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0145\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0160\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0233\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0148\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0194\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0138\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0190\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0025\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0033\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0124\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0073\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0043\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0035\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0054\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0144\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0075\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0090\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0159\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0047\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0318\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0449\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0118\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0210\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0030\n",
      "âœ… Epoch 2 complete | Loss: 94.7624 | Val Dice: 0.9852\n",
      "âš ï¸ No improvement. Patience: 1/10\n",
      "\n",
      "ğŸ” Epoch 3/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0056\n",
      "   â†ª Batch 100/5741 | Loss: 0.0029\n",
      "   â†ª Batch 200/5741 | Loss: 0.0075\n",
      "   â†ª Batch 300/5741 | Loss: 0.0050\n",
      "   â†ª Batch 400/5741 | Loss: 0.0020\n",
      "   â†ª Batch 500/5741 | Loss: 0.1898\n",
      "   â†ª Batch 600/5741 | Loss: 0.0213\n",
      "   â†ª Batch 700/5741 | Loss: 0.0073\n",
      "   â†ª Batch 800/5741 | Loss: 0.0143\n",
      "   â†ª Batch 900/5741 | Loss: 0.0101\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0325\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0920\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0318\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0091\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0167\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0265\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0041\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0015\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0213\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0255\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0058\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0051\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0386\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0119\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0025\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0036\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0114\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0124\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0050\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0027\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0104\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0015\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0075\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0063\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0167\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0233\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0117\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0019\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0049\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0245\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0053\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0099\n",
      "   â†ª Batch 4300/5741 | Loss: 0.1051\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0154\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0161\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0025\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0186\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0116\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0050\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0210\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0075\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0386\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0096\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0058\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0051\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0024\n",
      "âœ… Epoch 3 complete | Loss: 79.8232 | Val Dice: 0.9882\n",
      "ğŸ’¾ Best model saved with Dice: 0.9882\n",
      "\n",
      "ğŸ” Epoch 4/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0032\n",
      "   â†ª Batch 100/5741 | Loss: 0.0180\n",
      "   â†ª Batch 200/5741 | Loss: 0.0184\n",
      "   â†ª Batch 300/5741 | Loss: 0.0171\n",
      "   â†ª Batch 400/5741 | Loss: 0.0178\n",
      "   â†ª Batch 500/5741 | Loss: 0.0119\n",
      "   â†ª Batch 600/5741 | Loss: 0.0829\n",
      "   â†ª Batch 700/5741 | Loss: 0.0014\n",
      "   â†ª Batch 800/5741 | Loss: 0.0115\n",
      "   â†ª Batch 900/5741 | Loss: 0.0123\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0029\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0105\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0314\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0057\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0028\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0019\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0064\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0374\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0314\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0122\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0075\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0007\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0009\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0037\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0005\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0021\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0072\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0031\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0011\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0157\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0025\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0061\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0039\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0152\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0006\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0038\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0097\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0354\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0090\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0086\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0035\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0214\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0080\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0202\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0044\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0139\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0046\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0062\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0049\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0048\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0044\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0067\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0021\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0196\n",
      "âœ… Epoch 4 complete | Loss: 72.5151 | Val Dice: 0.9864\n",
      "âš ï¸ No improvement. Patience: 1/10\n",
      "\n",
      "ğŸ” Epoch 5/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0383\n",
      "   â†ª Batch 100/5741 | Loss: 0.0004\n",
      "   â†ª Batch 200/5741 | Loss: 0.0086\n",
      "   â†ª Batch 300/5741 | Loss: 0.0139\n",
      "   â†ª Batch 400/5741 | Loss: 0.0012\n",
      "   â†ª Batch 500/5741 | Loss: 0.0111\n",
      "   â†ª Batch 600/5741 | Loss: 0.0212\n",
      "   â†ª Batch 700/5741 | Loss: 0.0063\n",
      "   â†ª Batch 800/5741 | Loss: 0.0118\n",
      "   â†ª Batch 900/5741 | Loss: 0.0086\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0051\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0095\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0035\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0063\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0118\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0110\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0122\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0096\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0080\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0109\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0018\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0075\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0245\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0209\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0093\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0132\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0070\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0062\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0074\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0527\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0151\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0093\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0062\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0063\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0153\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0033\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0062\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0401\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0028\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0323\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0023\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0079\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0132\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0213\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0032\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0032\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0051\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0030\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0065\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0156\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0301\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0071\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0289\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0034\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0103\n",
      "âœ… Epoch 5 complete | Loss: 67.5935 | Val Dice: 0.9863\n",
      "âš ï¸ No improvement. Patience: 2/10\n",
      "\n",
      "ğŸ” Epoch 6/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0046\n",
      "   â†ª Batch 100/5741 | Loss: 0.0065\n",
      "   â†ª Batch 200/5741 | Loss: 0.0027\n",
      "   â†ª Batch 300/5741 | Loss: 0.0095\n",
      "   â†ª Batch 400/5741 | Loss: 0.0141\n",
      "   â†ª Batch 500/5741 | Loss: 0.0087\n",
      "   â†ª Batch 600/5741 | Loss: 0.0060\n",
      "   â†ª Batch 700/5741 | Loss: 0.0259\n",
      "   â†ª Batch 800/5741 | Loss: 0.0038\n",
      "   â†ª Batch 900/5741 | Loss: 0.0040\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0031\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0125\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0003\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0096\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0224\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0089\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0058\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0127\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0043\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0011\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0248\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0020\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0163\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0103\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0052\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0014\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0237\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0003\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0061\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0056\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0179\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0205\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0042\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0107\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0333\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0199\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0062\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0096\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0184\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0135\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0011\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0080\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0029\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0041\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0241\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0188\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0369\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0071\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0027\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0103\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0030\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0046\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0003\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0196\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0116\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0142\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0085\n",
      "âœ… Epoch 6 complete | Loss: 64.7916 | Val Dice: 0.9803\n",
      "âš ï¸ No improvement. Patience: 3/10\n",
      "\n",
      "ğŸ” Epoch 7/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0026\n",
      "   â†ª Batch 100/5741 | Loss: 0.0003\n",
      "   â†ª Batch 200/5741 | Loss: 0.0324\n",
      "   â†ª Batch 300/5741 | Loss: 0.0126\n",
      "   â†ª Batch 400/5741 | Loss: 0.0502\n",
      "   â†ª Batch 500/5741 | Loss: 0.0036\n",
      "   â†ª Batch 600/5741 | Loss: 0.0201\n",
      "   â†ª Batch 700/5741 | Loss: 0.0015\n",
      "   â†ª Batch 800/5741 | Loss: 0.0031\n",
      "   â†ª Batch 900/5741 | Loss: 0.0110\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0062\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0036\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0083\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0028\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0622\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0341\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0060\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0072\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0208\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0227\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0031\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0070\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0221\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0227\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0074\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0152\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0090\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0083\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0192\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0004\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0216\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0103\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0106\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0358\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0042\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0141\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0154\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0059\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0072\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0197\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0106\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0093\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0127\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0033\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0104\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0095\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0028\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0101\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0417\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0066\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0086\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0011\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0003\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0023\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0038\n",
      "âœ… Epoch 7 complete | Loss: 62.1531 | Val Dice: 0.9874\n",
      "âš ï¸ No improvement. Patience: 4/10\n",
      "\n",
      "ğŸ” Epoch 8/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0196\n",
      "   â†ª Batch 100/5741 | Loss: 0.0039\n",
      "   â†ª Batch 200/5741 | Loss: 0.0110\n",
      "   â†ª Batch 300/5741 | Loss: 0.0044\n",
      "   â†ª Batch 400/5741 | Loss: 0.0302\n",
      "   â†ª Batch 500/5741 | Loss: 0.0181\n",
      "   â†ª Batch 600/5741 | Loss: 0.0069\n",
      "   â†ª Batch 700/5741 | Loss: 0.0183\n",
      "   â†ª Batch 800/5741 | Loss: 0.0091\n",
      "   â†ª Batch 900/5741 | Loss: 0.0143\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0073\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0018\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0011\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0079\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0157\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0179\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0067\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0025\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0038\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0018\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0089\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0095\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0036\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0063\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0137\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0082\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0007\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0033\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0093\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0100\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0035\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0045\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0007\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0055\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0108\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0091\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0320\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0035\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0127\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0010\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0098\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0135\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0276\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0114\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0018\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0120\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0013\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0099\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0098\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0107\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0020\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0264\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0009\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0033\n",
      "âœ… Epoch 8 complete | Loss: 59.4653 | Val Dice: 0.9837\n",
      "âš ï¸ No improvement. Patience: 5/10\n",
      "\n",
      "ğŸ” Epoch 9/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0176\n",
      "   â†ª Batch 100/5741 | Loss: 0.0103\n",
      "   â†ª Batch 200/5741 | Loss: 0.0515\n",
      "   â†ª Batch 300/5741 | Loss: 0.0088\n",
      "   â†ª Batch 400/5741 | Loss: 0.0588\n",
      "   â†ª Batch 500/5741 | Loss: 0.0117\n",
      "   â†ª Batch 600/5741 | Loss: 0.0101\n",
      "   â†ª Batch 700/5741 | Loss: 0.0048\n",
      "   â†ª Batch 800/5741 | Loss: 0.0169\n",
      "   â†ª Batch 900/5741 | Loss: 0.0003\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0055\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0097\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0047\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0110\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0480\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0248\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0103\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0339\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0358\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0230\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0369\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0055\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0030\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0052\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0019\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0028\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0254\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0503\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0475\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0044\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0053\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0045\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0068\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0036\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0021\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0063\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0119\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0024\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0040\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0007\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0026\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0245\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0058\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0031\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0053\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0062\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0107\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0036\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0012\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0030\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0051\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0057\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0005\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0223\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0095\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0124\n",
      "âœ… Epoch 9 complete | Loss: 57.9183 | Val Dice: 0.9778\n",
      "âš ï¸ No improvement. Patience: 6/10\n",
      "\n",
      "ğŸ” Epoch 10/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0005\n",
      "   â†ª Batch 100/5741 | Loss: 0.0003\n",
      "   â†ª Batch 200/5741 | Loss: 0.0151\n",
      "   â†ª Batch 300/5741 | Loss: 0.0196\n",
      "   â†ª Batch 400/5741 | Loss: 0.0086\n",
      "   â†ª Batch 500/5741 | Loss: 0.0158\n",
      "   â†ª Batch 600/5741 | Loss: 0.0054\n",
      "   â†ª Batch 700/5741 | Loss: 0.0026\n",
      "   â†ª Batch 800/5741 | Loss: 0.0278\n",
      "   â†ª Batch 900/5741 | Loss: 0.0044\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0049\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0025\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0042\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0120\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0107\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0034\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0013\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0024\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0137\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0033\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0009\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0118\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0175\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0115\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0052\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0002\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0036\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0127\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0291\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0186\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0047\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0064\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0019\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0165\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0017\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0018\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0471\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0047\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0012\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0111\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0086\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0049\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0017\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0202\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0006\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0010\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0192\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0066\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0036\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0020\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0640\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0434\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0026\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0060\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0031\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0077\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0214\n",
      "âœ… Epoch 10 complete | Loss: 56.6357 | Val Dice: 0.9838\n",
      "âš ï¸ No improvement. Patience: 7/10\n",
      "\n",
      "ğŸ” Epoch 11/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0053\n",
      "   â†ª Batch 100/5741 | Loss: 0.0047\n",
      "   â†ª Batch 200/5741 | Loss: 0.0034\n",
      "   â†ª Batch 300/5741 | Loss: 0.0357\n",
      "   â†ª Batch 400/5741 | Loss: 0.0077\n",
      "   â†ª Batch 500/5741 | Loss: 0.0055\n",
      "   â†ª Batch 600/5741 | Loss: 0.0194\n",
      "   â†ª Batch 700/5741 | Loss: 0.0083\n",
      "   â†ª Batch 800/5741 | Loss: 0.0017\n",
      "   â†ª Batch 900/5741 | Loss: 0.0077\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0141\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0005\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0067\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0152\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0084\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0045\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0206\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0016\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0235\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0023\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0006\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0102\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0030\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0021\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0630\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0061\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0016\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0041\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0066\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0023\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0095\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0019\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0026\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0438\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0051\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0096\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0045\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0023\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0084\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0036\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0057\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0072\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0089\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0021\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0082\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0070\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0078\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0001\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0044\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0016\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0353\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0104\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0001\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0040\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0051\n",
      "âœ… Epoch 11 complete | Loss: 54.8271 | Val Dice: 0.9874\n",
      "âš ï¸ No improvement. Patience: 8/10\n",
      "\n",
      "ğŸ” Epoch 12/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0026\n",
      "   â†ª Batch 100/5741 | Loss: 0.0087\n",
      "   â†ª Batch 200/5741 | Loss: 0.0065\n",
      "   â†ª Batch 300/5741 | Loss: 0.0059\n",
      "   â†ª Batch 400/5741 | Loss: 0.0081\n",
      "   â†ª Batch 500/5741 | Loss: 0.0028\n",
      "   â†ª Batch 600/5741 | Loss: 0.0024\n",
      "   â†ª Batch 700/5741 | Loss: 0.0119\n",
      "   â†ª Batch 800/5741 | Loss: 0.0076\n",
      "   â†ª Batch 900/5741 | Loss: 0.0174\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0153\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0082\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0216\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0024\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0116\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0004\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0135\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0067\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0165\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0099\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0081\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0078\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0251\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0022\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0091\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0033\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0173\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0150\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0007\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0118\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0113\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0050\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0060\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0089\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0172\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0270\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0111\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0179\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0056\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0139\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0045\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0062\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0199\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0038\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0046\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0124\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0095\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0016\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0033\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0026\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0099\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0081\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0020\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0572\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0029\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0017\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0029\n",
      "âœ… Epoch 12 complete | Loss: 53.6606 | Val Dice: 0.9850\n",
      "âš ï¸ No improvement. Patience: 9/10\n",
      "\n",
      "ğŸ” Epoch 13/100\n",
      "   â†ª Batch 0/5741 | Loss: 0.0106\n",
      "   â†ª Batch 100/5741 | Loss: 0.0005\n",
      "   â†ª Batch 200/5741 | Loss: 0.0081\n",
      "   â†ª Batch 300/5741 | Loss: 0.0053\n",
      "   â†ª Batch 400/5741 | Loss: 0.0010\n",
      "   â†ª Batch 500/5741 | Loss: 0.0135\n",
      "   â†ª Batch 600/5741 | Loss: 0.0003\n",
      "   â†ª Batch 700/5741 | Loss: 0.0189\n",
      "   â†ª Batch 800/5741 | Loss: 0.0182\n",
      "   â†ª Batch 900/5741 | Loss: 0.0073\n",
      "   â†ª Batch 1000/5741 | Loss: 0.0001\n",
      "   â†ª Batch 1100/5741 | Loss: 0.0006\n",
      "   â†ª Batch 1200/5741 | Loss: 0.0026\n",
      "   â†ª Batch 1300/5741 | Loss: 0.0024\n",
      "   â†ª Batch 1400/5741 | Loss: 0.0039\n",
      "   â†ª Batch 1500/5741 | Loss: 0.0047\n",
      "   â†ª Batch 1600/5741 | Loss: 0.0136\n",
      "   â†ª Batch 1700/5741 | Loss: 0.0050\n",
      "   â†ª Batch 1800/5741 | Loss: 0.0034\n",
      "   â†ª Batch 1900/5741 | Loss: 0.0087\n",
      "   â†ª Batch 2000/5741 | Loss: 0.0045\n",
      "   â†ª Batch 2100/5741 | Loss: 0.0131\n",
      "   â†ª Batch 2200/5741 | Loss: 0.0035\n",
      "   â†ª Batch 2300/5741 | Loss: 0.0018\n",
      "   â†ª Batch 2400/5741 | Loss: 0.0069\n",
      "   â†ª Batch 2500/5741 | Loss: 0.0002\n",
      "   â†ª Batch 2600/5741 | Loss: 0.0207\n",
      "   â†ª Batch 2700/5741 | Loss: 0.0059\n",
      "   â†ª Batch 2800/5741 | Loss: 0.0017\n",
      "   â†ª Batch 2900/5741 | Loss: 0.0072\n",
      "   â†ª Batch 3000/5741 | Loss: 0.0004\n",
      "   â†ª Batch 3100/5741 | Loss: 0.0225\n",
      "   â†ª Batch 3200/5741 | Loss: 0.0028\n",
      "   â†ª Batch 3300/5741 | Loss: 0.0012\n",
      "   â†ª Batch 3400/5741 | Loss: 0.0091\n",
      "   â†ª Batch 3500/5741 | Loss: 0.0611\n",
      "   â†ª Batch 3600/5741 | Loss: 0.0156\n",
      "   â†ª Batch 3700/5741 | Loss: 0.0016\n",
      "   â†ª Batch 3800/5741 | Loss: 0.0129\n",
      "   â†ª Batch 3900/5741 | Loss: 0.0007\n",
      "   â†ª Batch 4000/5741 | Loss: 0.0209\n",
      "   â†ª Batch 4100/5741 | Loss: 0.0038\n",
      "   â†ª Batch 4200/5741 | Loss: 0.0977\n",
      "   â†ª Batch 4300/5741 | Loss: 0.0029\n",
      "   â†ª Batch 4400/5741 | Loss: 0.0014\n",
      "   â†ª Batch 4500/5741 | Loss: 0.0024\n",
      "   â†ª Batch 4600/5741 | Loss: 0.0103\n",
      "   â†ª Batch 4700/5741 | Loss: 0.0104\n",
      "   â†ª Batch 4800/5741 | Loss: 0.0009\n",
      "   â†ª Batch 4900/5741 | Loss: 0.0076\n",
      "   â†ª Batch 5000/5741 | Loss: 0.0002\n",
      "   â†ª Batch 5100/5741 | Loss: 0.0014\n",
      "   â†ª Batch 5200/5741 | Loss: 0.0131\n",
      "   â†ª Batch 5300/5741 | Loss: 0.0129\n",
      "   â†ª Batch 5400/5741 | Loss: 0.0024\n",
      "   â†ª Batch 5500/5741 | Loss: 0.0032\n",
      "   â†ª Batch 5600/5741 | Loss: 0.0042\n",
      "   â†ª Batch 5700/5741 | Loss: 0.0218\n",
      "âœ… Epoch 13 complete | Loss: 52.4541 | Val Dice: 0.9856\n",
      "âš ï¸ No improvement. Patience: 10/10\n",
      "â¹ï¸ Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "att_unet = AttentionUNet3D()\n",
    "train_model(att_unet, train_loader, val_loader, name=\"att_unet3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d398cba",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ensemble Prediction and Visualization\n",
    " This combines predictions from U-Net and Attention U-Net, averages the logits, and visualizes the ensemble result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86112439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ensemble_predict(x):\n",
    "    model1 = UNet3D().to(DEVICE)\n",
    "    model2 = AttentionUNet3D().to(DEVICE)\n",
    "    model1.load_state_dict(torch.load(\"unet3d_best.pth\"))\n",
    "    model2.load_state_dict(torch.load(\"att_unet3d_best.pth\"))\n",
    "    model1.eval(); model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out1 = model1(x)\n",
    "        out2 = model2(x)\n",
    "        avg_out = (out1 + out2) / 2\n",
    "        pred = torch.argmax(avg_out, dim=1)\n",
    "    return pred\n",
    "\n",
    "def visualize_ensemble(val_loader):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    pred = ensemble_predict(x)\n",
    "\n",
    "    mid_slice = x.shape[-1] // 2\n",
    "    for i in range(min(2, x.shape[0])):  # show 2 examples max\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(x[i][0, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Input (FLAIR)\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(y[i, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred[i, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Ensemble Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "visualize_ensemble(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b165cb",
   "metadata": {},
   "source": [
    "## Export to TorchScript & ONNX\n",
    "This cell lets you export both U-Net and Attention U-Net models for deployment (in PyTorch or external tools like TensorRT or ONNX Runtime).\n",
    "\n",
    "âœ… unet3d.pt and att_unet3d.pt â†’ usable in PyTorch mobile or C++ inference\n",
    "\n",
    "âœ… unet3d.onnx and att_unet3d.onnx â†’ compatible with ONNX Runtime, TensorRT, OpenVINO, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "def export_model(model_class, weight_path, name=\"unet3d\", input_shape=(1, 4, 128, 128, 64)):\n",
    "    model = model_class().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    model.eval()\n",
    "\n",
    "    dummy_input = torch.randn(input_shape).to(DEVICE)\n",
    "\n",
    "    # âœ… Export to TorchScript\n",
    "    traced = torch.jit.trace(model, dummy_input)\n",
    "    traced.save(f\"{name}.pt\")\n",
    "    print(f\"âœ… TorchScript saved: {name}.pt\")\n",
    "\n",
    "    # âœ… Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        f\"{name}.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"âœ… ONNX saved: {name}.onnx\")\n",
    "\n",
    "# Export both models\n",
    "export_model(UNet3D, \"unet3d_best.pth\", name=\"unet3d\")\n",
    "export_model(AttentionUNet3D, \"att_unet3d_best.pth\", name=\"att_unet3d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d462fd4",
   "metadata": {},
   "source": [
    "#  Dice Score & Loss Tracking Plot\n",
    "Weâ€™ll modify the training function slightly to return per-epoch loss and Dice values, and then plot them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, name, epochs=100):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    best_dice = 0\n",
    "    patience = 10\n",
    "    trigger = 0\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_dice\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"[{name}] Epoch {epoch}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        dice = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with autocast():\n",
    "                    pred = torch.argmax(model(x), dim=1)\n",
    "                dice += dice_coeff(pred.cpu(), y.cpu())\n",
    "\n",
    "        avg_dice = dice / len(val_loader)\n",
    "        history[\"train_loss\"].append(total_loss)\n",
    "        history[\"val_dice\"].append(avg_dice.item())\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss: {total_loss:.4f} | Val Dice: {avg_dice:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "            print(f\"âœ… Saved best model with Dice: {best_dice:.4f}\")\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                print(\"â¹ï¸ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_history(history, title=\"Training History\"):\n",
    "    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\", marker=\"x\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_unet = train_model(UNet3D(), train_loader, val_loader, name=\"unet3d\", epochs=50)\n",
    "plot_history(history_unet, title=\"U-Net 3D\")\n",
    "\n",
    "history_attn = train_model(AttentionUNet3D(), train_loader, val_loader, name=\"att_unet3d\", epochs=50)\n",
    "plot_history(history_attn, title=\"Attention U-Net 3D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00059ae3",
   "metadata": {},
   "source": [
    "# Inference on a New .nii.gz Patient\n",
    "This lets you run inference on a single new BraTS-format patient folder and visualize the middle slice result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d193fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def run_inference_on_patient(patient_dir, model_path, model_class, patch_depth=64):\n",
    "    flair = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_flair.nii.gz\")).get_fdata()\n",
    "    t1 = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t1.nii.gz\")).get_fdata()\n",
    "    t1ce = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t1ce.nii.gz\")).get_fdata()\n",
    "    t2 = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t2.nii.gz\")).get_fdata()\n",
    "\n",
    "    # Stack channels: (4, H, W, D)\n",
    "    image = np.stack([flair, t1, t1ce, t2], axis=0).astype(np.float32)\n",
    "    image = torch.tensor(image).unsqueeze(0).to(DEVICE)  # (1, 4, H, W, D)\n",
    "\n",
    "    model = model_class().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prediction = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    return prediction  # shape: (H, W, D)\n",
    "\n",
    "# ğŸ” Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_patient_prediction(patient_dir, prediction):\n",
    "    flair_path = os.path.join(patient_dir, os.path.basename(patient_dir) + \"_flair.nii.gz\")\n",
    "    flair = nib.load(flair_path).get_fdata()\n",
    "\n",
    "    mid_slice = flair.shape[-1] // 2\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(flair[:, :, mid_slice], cmap='gray')\n",
    "    plt.title(\"FLAIR Input\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(prediction[:, :, mid_slice], cmap='gray')\n",
    "    plt.title(\"Predicted Segmentation\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Inference on a single new patient folder\n",
    "new_patient = r\"D:\\BraTS2021_Training_Data\\BraTS2021_00005\"  # replace with your path\n",
    "pred = run_inference_on_patient(new_patient, \"unet3d_best.pth\", UNet3D)\n",
    "visualize_patient_prediction(new_patient, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bceae8",
   "metadata": {},
   "source": [
    "# ğŸ’¾ Save Predicted Mask as .nii.gz\n",
    "\n",
    "This is useful when you want to visualize or process the segmentation result in tools like 3D Slicer, ITK-SNAP, or use for clinical post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb507e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_as_nifti(prediction, reference_nifti_path, output_path):\n",
    "    \"\"\"\n",
    "    Saves a 3D numpy array (prediction) as a .nii.gz file using the affine and header from a reference image.\n",
    "    \"\"\"\n",
    "    reference_nifti = nib.load(reference_nifti_path)\n",
    "    pred_nifti = nib.Nifti1Image(prediction.astype(np.uint8), affine=reference_nifti.affine, header=reference_nifti.header)\n",
    "    nib.save(pred_nifti, output_path)\n",
    "    print(f\"âœ… Saved predicted mask as: {output_path}\")\n",
    "\n",
    "\n",
    "# Assuming you already have `pred` from the previous inference\n",
    "ref_img_path = os.path.join(new_patient, os.path.basename(new_patient) + \"_flair.nii.gz\")\n",
    "output_path = \"BraTS2021_00005_predicted_mask.nii.gz\"\n",
    "\n",
    "save_prediction_as_nifti(pred, ref_img_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44243c2",
   "metadata": {},
   "source": [
    "# Class Distribution Analysis\n",
    "This helps you analyze label imbalance across the segmentation masks, which is common in medical datasets like BraTS (e.g., large background, small tumor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac47869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Function to Analyze Label Frequencies:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_class_distribution(dataset, num_classes=4):\n",
    "    class_counts = np.zeros(num_classes)\n",
    "\n",
    "    for i in range(min(100, len(dataset))):  # analyze first 100 samples to save RAM\n",
    "        _, mask = dataset[i]\n",
    "        unique, counts = np.unique(mask.numpy(), return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            if u < num_classes:\n",
    "                class_counts[int(u)] += c\n",
    "\n",
    "    total = class_counts.sum()\n",
    "    percentages = 100 * class_counts / total\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}: {int(class_counts[cls])} voxels ({percentages[cls]:.2f}%)\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(num_classes), percentages)\n",
    "    plt.xticks(range(num_classes))\n",
    "    plt.title(\"Class Distribution (%)\")\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c0898",
   "metadata": {},
   "source": [
    "# Model Summary with torchinfo\n",
    "This helps visualize the architecture, layer-wise output shapes, and parameter counts for each model. Very useful for documentation and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def show_model_summary(model_class, input_shape=(1, 4, 128, 128, 64), name=\"Model\"):\n",
    "    print(f\"\\nğŸ“Š Summary for {name}\")\n",
    "    model = model_class().to(DEVICE)\n",
    "    summary(model, input_size=input_shape, depth=3, col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "    \n",
    "show_model_summary(UNet3D, name=\"U-Net 3D\")\n",
    "show_model_summary(AttentionUNet3D, name=\"Attention U-Net 3D\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26ebf2",
   "metadata": {},
   "source": [
    "## âœ… Summary & Deployment Tips\n",
    "- Best Dice model saved as `.pth`\n",
    "- Deployment-ready formats: `.pt` (TorchScript), `.onnx`\n",
    "- Use ONNX for ONNX Runtime, TensorRT, or OpenVINO.\n",
    "- Ideal for mobile, embedded, or web deployment.\n",
    "\n",
    "**Next step:** Try converting the ONNX model into TensorRT or integrating in a simple Flask demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
