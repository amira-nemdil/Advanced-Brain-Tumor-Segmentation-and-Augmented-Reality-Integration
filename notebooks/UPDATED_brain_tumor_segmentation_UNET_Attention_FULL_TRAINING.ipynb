{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579c21af",
   "metadata": {},
   "source": [
    "# 🧠 Brain Tumor Segmentation Using 3D U-Net and Attention U-Net (PyTorch)\n",
    "### Advanced Deep Learning Pipeline for MRI Segmentation\n",
    "\n",
    "This notebook presents a complete deep learning pipeline for 3D brain tumor segmentation from MRI images using U-Net and Attention U-Net architectures.\n",
    "\n",
    "**Features:**\n",
    "- Data preprocessing and loading for multi-modal MRI\n",
    "- Custom PyTorch dataset and data loader\n",
    "- Implementation of U-Net and Attention U-Net\n",
    "- Advanced training loop with early stopping and learning rate scheduling\n",
    "- Quantitative evaluation (Dice, loss curves) and qualitative visualizations\n",
    "- Easily adaptable for research, internship, and clinical exploration\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c2d16",
   "metadata": {},
   "source": [
    "## ⚙️ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d69666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('✅ Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a955e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available: True\n",
      "🧠 Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ CUDA available:\", torch.cuda.is_available())\n",
    "print(\"🧠 Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44235593",
   "metadata": {},
   "source": [
    "## 📂 Dataset Loader: Patch-wise, Lazy Loading from `.nii.gz` (BraTS2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d72b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BraTS 2023 Patch-based 3D Dataset\n",
    "# This dataset class loads 3D patches from BraTS patient directories.\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os, random\n",
    "\n",
    "class BraTSPatch3DDataset(Dataset):\n",
    "    def __init__(self, patient_dirs, patch_depth=64, augment=False):\n",
    "        self.patch_depth = patch_depth\n",
    "        self.augment = augment\n",
    "        self.patients = []       # Preloaded patients: (image, seg)\n",
    "        self.patch_index = []    # List of (patient_idx, z_start)\n",
    "\n",
    "        for i, p in enumerate(patient_dirs):\n",
    "            pid = os.path.basename(p)\n",
    "\n",
    "            def load_modality(name):\n",
    "                path = os.path.join(p, f\"{pid}_{name}.nii.gz\")\n",
    "                return nib.load(path).get_fdata(dtype=np.float32)\n",
    "\n",
    "            # Load and normalize once\n",
    "            flair = load_modality(\"flair\")\n",
    "            t1 = load_modality(\"t1\")\n",
    "            t1ce = load_modality(\"t1ce\")\n",
    "            t2 = load_modality(\"t2\")\n",
    "            seg = load_modality(\"seg\")\n",
    "\n",
    "            image = np.stack([flair, t1, t1ce, t2], axis=0)  # Shape: (4, H, W, D)\n",
    "            for c in range(4):\n",
    "                img = image[c]\n",
    "                image[c] = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "\n",
    "            self.patients.append((image, seg))\n",
    "\n",
    "            # Create patch indices\n",
    "            depth = image.shape[-1]\n",
    "            for z in range(0, depth - patch_depth, patch_depth // 2):\n",
    "                self.patch_index.append((i, z))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx, z = self.patch_index[idx]\n",
    "        image, seg = self.patients[patient_idx]\n",
    "\n",
    "        patch_img = image[:, :, :, z:z+self.patch_depth]\n",
    "        patch_seg = seg[:, :, z:z+self.patch_depth]\n",
    "\n",
    "        patch_img = torch.tensor(patch_img.copy(), dtype=torch.float32)\n",
    "        patch_seg = torch.tensor(patch_seg.copy(), dtype=torch.long)\n",
    "\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            patch_img = torch.flip(patch_img, dims=[3])\n",
    "            patch_seg = torch.flip(patch_seg, dims=[2])\n",
    "\n",
    "        return patch_img, patch_seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823361e3",
   "metadata": {},
   "source": [
    "## 🔀 Dataset Split: Train & Validation\n",
    "We use an 80/20 split to train and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27317d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully\n",
      "Train patients: 1000\n",
      "Val patients: 251\n",
      "Train samples (patches): 8000\n",
      "Val samples (patches): 2008\n"
     ]
    }
   ],
   "source": [
    "# Set your BraTS2021 root directory\n",
    "root_dir = r\"C:\\Users\\amira\\Downloads\\BraTS2021_Training_Data\"\n",
    "\n",
    "# Gather patient directories\n",
    "patient_dirs = sorted([\n",
    "    os.path.join(root_dir, d)\n",
    "    for d in os.listdir(root_dir)\n",
    "    if os.path.isdir(os.path.join(root_dir, d))\n",
    "])\n",
    "random.shuffle(patient_dirs)  # Shuffle for randomness\n",
    "\n",
    "# Patient-level 80/20 split to avoid data leakage\n",
    "split_idx = int(0.8 * len(patient_dirs))\n",
    "train_patients = patient_dirs[:split_idx]\n",
    "val_patients = patient_dirs[split_idx:]\n",
    "\n",
    "# Initialize datasets\n",
    "\n",
    "class BraTSPatch3DDataset(Dataset):\n",
    "    def __init__(self, patient_dirs, patch_depth=64, augment=False):\n",
    "        self.patient_dirs = patient_dirs\n",
    "        self.patch_depth = patch_depth\n",
    "        self.augment = augment\n",
    "        self.patch_index = []  # Stores (patient_index, z_start)\n",
    "\n",
    "        # Build patch index only for tumor-containing patches\n",
    "        for i, p in enumerate(patient_dirs):\n",
    "            seg_path = os.path.join(p, os.path.basename(p) + '_seg.nii.gz')\n",
    "            seg = nib.load(seg_path).get_fdata()\n",
    "            depth = seg.shape[-1]\n",
    "\n",
    "            for z in range(0, depth - patch_depth, patch_depth // 2):\n",
    "                patch = seg[:, :, z:z+patch_depth]\n",
    "                if np.max(patch) > 0:  # ✅ Skip background-only patches\n",
    "                    self.patch_index.append((i, z))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx, z = self.patch_index[idx]\n",
    "        p = self.patient_dirs[patient_idx]\n",
    "        pid = os.path.basename(p)\n",
    "\n",
    "        def load_modality(name):\n",
    "            path = os.path.join(p, f\"{pid}_{name}.nii.gz\")\n",
    "            return nib.load(path).get_fdata(dtype=np.float32)\n",
    "\n",
    "        # Load 4 MRI modalities\n",
    "        flair = load_modality(\"flair\")\n",
    "        t1 = load_modality(\"t1\")\n",
    "        t1ce = load_modality(\"t1ce\")\n",
    "        t2 = load_modality(\"t2\")\n",
    "        seg = load_modality(\"seg\")\n",
    "\n",
    "        # Extract 3D patch: (C, H, W, D)\n",
    "        image = np.stack([flair, t1, t1ce, t2], axis=0)[:, :, :, z:z+self.patch_depth]\n",
    "        mask = seg[:, :, z:z+self.patch_depth]\n",
    "\n",
    "        # Normalize each channel independently\n",
    "        for i in range(4):\n",
    "            img = image[i]\n",
    "            image[i] = (img - np.mean(img)) / (np.std(img) + 1e-5)\n",
    "\n",
    "        # Convert to tensors\n",
    "        image = torch.tensor(image.copy(), dtype=torch.float32)\n",
    "        mask = torch.tensor(mask.copy(), dtype=torch.long)\n",
    "\n",
    "        # Optional horizontal flip\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            image = torch.flip(image, dims=[3])  # flip in depth\n",
    "            mask = torch.flip(mask, dims=[2])\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "train_dataset = BraTSPatch3DDataset(train_patients, patch_depth=32, augment=True)\n",
    "val_dataset = BraTSPatch3DDataset(val_patients, patch_depth=32, augment=False)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Display info\n",
    "print(\"✅ Dataset loaded successfully\")\n",
    "print(\"Train patients:\", len(train_patients))\n",
    "print(\"Val patients:\", len(val_patients))\n",
    "print(\"Train samples (patches):\", len(train_dataset))\n",
    "print(\"Val samples (patches):\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c730cd2",
   "metadata": {},
   "source": [
    "## 🧠 U-Net 3D and Attention U-Net3D Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba4f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.enc1 = block(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = block(32, 64)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = block(64, 128)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, 2)\n",
    "        self.dec2 = block(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, 2)\n",
    "        self.dec1 = block(64, 32)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.dec2(torch.cat([self.up2(b), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return self.out_conv(d1)\n",
    "\n",
    "class AttentionBlock3D(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv3d(F_g, F_int, 1), nn.BatchNorm3d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv3d(F_l, F_int, 1), nn.BatchNorm3d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv3d(F_int, 1, 1), nn.BatchNorm3d(1), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        return x * self.psi(self.relu(self.W_g(g) + self.W_x(x)))\n",
    "\n",
    "class AttentionUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):\n",
    "        super().__init__()\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.enc1 = block(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = block(32, 64)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = block(64, 128)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, 2)\n",
    "        self.att2 = AttentionBlock3D(64, 64, 32)\n",
    "        self.dec2 = block(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, 2)\n",
    "        self.att1 = AttentionBlock3D(32, 32, 16)\n",
    "        self.dec1 = block(64, 32)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.att2(self.up2(b), e2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.att1(self.up1(d2), e1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "        return self.out_conv(d1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c48c9",
   "metadata": {},
   "source": [
    "## 🧪 Training loop Function (AMP + EarlyStopping + Dice Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e013f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def dice_coeff(pred, target, eps=1e-6):\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    intersection = (pred == target).float().sum()\n",
    "    return (2. * intersection) / (pred.numel() + target.numel() + eps)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, name, epochs=100):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    best_dice = 0\n",
    "    patience = 10\n",
    "    trigger = 0\n",
    "    val_dices = []\n",
    "\n",
    "    print(f\"\\n⏳ Measuring first batch load time...\")\n",
    "    t0 = time.time()\n",
    "    _ = next(iter(train_loader))\n",
    "    print(f\"✅ First batch loaded in {time.time() - t0:.2f} seconds.\\n\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        print(f\"\\n🔁 Epoch {epoch}/{epochs}\")\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                # Ensure labels are within valid range\n",
    "                y = torch.clamp(y, 0, out.shape[1] - 1)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Batch progress logging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"   ↪ Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dice = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with autocast():\n",
    "                    pred = torch.argmax(model(x), dim=1)\n",
    "                dice += dice_coeff(pred.cpu(), y.cpu())\n",
    "\n",
    "        avg_dice = dice / len(val_loader)\n",
    "        val_dices.append(avg_dice.item())\n",
    "        print(f\"✅ Epoch {epoch} complete | Loss: {total_loss:.4f} | Val Dice: {avg_dice:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "            print(f\"💾 Best model saved with Dice: {best_dice:.4f}\")\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            print(f\"⚠️ No improvement. Patience: {trigger}/{patience}\")\n",
    "            if trigger >= patience:\n",
    "                print(\"⏹️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Plot Dice over epochs\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(val_dices, label=\"Validation Dice\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Dice Score\")\n",
    "    plt.title(f\"Dice Curve for {name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce991be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max label value in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _, y in train_loader:\n",
    "    labels.append(y.max().item())\n",
    "    break\n",
    "\n",
    "print(\"Max label value in dataset:\", max(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a024cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Measuring first batch load time...\n",
      "✅ First batch loaded in 0.69 seconds.\n",
      "\n",
      "\n",
      "🔁 Epoch 1/100\n",
      "   ↪ Batch 0/8000 | Loss: 1.2209\n",
      "   ↪ Batch 100/8000 | Loss: 0.7661\n",
      "   ↪ Batch 200/8000 | Loss: 0.5903\n",
      "   ↪ Batch 300/8000 | Loss: 0.4879\n",
      "   ↪ Batch 400/8000 | Loss: 0.4183\n",
      "   ↪ Batch 500/8000 | Loss: 0.3338\n",
      "   ↪ Batch 600/8000 | Loss: 0.2801\n",
      "   ↪ Batch 700/8000 | Loss: 0.2337\n",
      "   ↪ Batch 800/8000 | Loss: 0.1970\n",
      "   ↪ Batch 900/8000 | Loss: 0.1668\n",
      "   ↪ Batch 1000/8000 | Loss: 0.1438\n",
      "   ↪ Batch 1100/8000 | Loss: 0.1229\n",
      "   ↪ Batch 1200/8000 | Loss: 0.1149\n",
      "   ↪ Batch 1300/8000 | Loss: 0.1418\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0825\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0781\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0707\n",
      "   ↪ Batch 1700/8000 | Loss: 0.1077\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0469\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0413\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0955\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0440\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0575\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0303\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0420\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0228\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0208\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0271\n",
      "   ↪ Batch 2800/8000 | Loss: 0.1002\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0202\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0200\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0163\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0168\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0285\n",
      "   ↪ Batch 3400/8000 | Loss: 0.1411\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0248\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0587\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0084\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0315\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0112\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0099\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0168\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0070\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0177\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0408\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0140\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0047\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0203\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0052\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0162\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0074\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0133\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0092\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0066\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0105\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0316\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0025\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0251\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0088\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0022\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0139\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0123\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0019\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0103\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0059\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0018\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0015\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0131\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0041\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0014\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0261\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0063\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0068\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0098\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0099\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0009\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0100\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0049\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0362\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0131\n",
      "✅ Epoch 1 complete | Loss: 619.0871 | Val Dice: 0.9877\n",
      "💾 Best model saved with Dice: 0.9877\n",
      "\n",
      "🔁 Epoch 2/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0272\n",
      "   ↪ Batch 100/8000 | Loss: 0.0072\n",
      "   ↪ Batch 200/8000 | Loss: 0.0327\n",
      "   ↪ Batch 300/8000 | Loss: 0.0007\n",
      "   ↪ Batch 400/8000 | Loss: 0.0060\n",
      "   ↪ Batch 500/8000 | Loss: 0.0008\n",
      "   ↪ Batch 600/8000 | Loss: 0.0007\n",
      "   ↪ Batch 700/8000 | Loss: 0.0136\n",
      "   ↪ Batch 800/8000 | Loss: 0.0102\n",
      "   ↪ Batch 900/8000 | Loss: 0.0040\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0060\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0014\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0118\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0005\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0037\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0305\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0095\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0047\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0119\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0003\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0106\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0165\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0026\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0013\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0118\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0005\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0082\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0101\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0077\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0008\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0060\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0141\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0290\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0106\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0122\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0003\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0173\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0002\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0009\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0122\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0015\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0006\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0032\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0009\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0323\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0231\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0177\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0029\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0201\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0003\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0005\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0081\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0025\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0152\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0003\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0011\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0021\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0058\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0181\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0010\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0009\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0147\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0011\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0049\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0478\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0369\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0023\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0052\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0230\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0009\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0002\n",
      "✅ Epoch 2 complete | Loss: 85.8952 | Val Dice: 0.9916\n",
      "💾 Best model saved with Dice: 0.9916\n",
      "\n",
      "🔁 Epoch 3/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0081\n",
      "   ↪ Batch 100/8000 | Loss: 0.0374\n",
      "   ↪ Batch 200/8000 | Loss: 0.0011\n",
      "   ↪ Batch 300/8000 | Loss: 0.0026\n",
      "   ↪ Batch 400/8000 | Loss: 0.0082\n",
      "   ↪ Batch 500/8000 | Loss: 0.0097\n",
      "   ↪ Batch 600/8000 | Loss: 0.0006\n",
      "   ↪ Batch 700/8000 | Loss: 0.0017\n",
      "   ↪ Batch 800/8000 | Loss: 0.0024\n",
      "   ↪ Batch 900/8000 | Loss: 0.0018\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0022\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0230\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0033\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0032\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0446\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0133\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0208\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0104\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0013\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0104\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0049\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0258\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0004\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0088\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0869\n",
      "   ↪ Batch 2700/8000 | Loss: 0.1117\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0075\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0039\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0005\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0109\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0051\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0119\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0128\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0028\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0003\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0004\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0034\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0072\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0042\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0055\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0033\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0069\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0049\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0004\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0003\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0022\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0082\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0086\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0016\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0053\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0081\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0112\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0056\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0003\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0147\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0066\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0267\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0071\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0130\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0357\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0042\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0227\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0026\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0153\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0077\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0002\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0000\n",
      "✅ Epoch 3 complete | Loss: 74.4646 | Val Dice: 0.9918\n",
      "💾 Best model saved with Dice: 0.9918\n",
      "\n",
      "🔁 Epoch 4/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0001\n",
      "   ↪ Batch 100/8000 | Loss: 0.0044\n",
      "   ↪ Batch 200/8000 | Loss: 0.0002\n",
      "   ↪ Batch 300/8000 | Loss: 0.0033\n",
      "   ↪ Batch 400/8000 | Loss: 0.0235\n",
      "   ↪ Batch 500/8000 | Loss: 0.0023\n",
      "   ↪ Batch 600/8000 | Loss: 0.0161\n",
      "   ↪ Batch 700/8000 | Loss: 0.0026\n",
      "   ↪ Batch 800/8000 | Loss: 0.0009\n",
      "   ↪ Batch 900/8000 | Loss: 0.0024\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0092\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0111\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0173\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0183\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0175\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0144\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0040\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0004\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0003\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0012\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0026\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0049\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0040\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0051\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0255\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0005\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0302\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0080\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0032\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0055\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0067\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0091\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0130\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0078\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0064\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0121\n",
      "   ↪ Batch 4100/8000 | Loss: 0.1155\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0463\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0073\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0019\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0122\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0076\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0046\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0036\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0005\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0088\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0089\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0030\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0020\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0006\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0060\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0166\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0029\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0034\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0010\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0233\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0090\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0107\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0021\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0056\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0156\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0137\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0108\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0004\n",
      "✅ Epoch 4 complete | Loss: 67.2419 | Val Dice: 0.9932\n",
      "💾 Best model saved with Dice: 0.9932\n",
      "\n",
      "🔁 Epoch 5/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0495\n",
      "   ↪ Batch 100/8000 | Loss: 0.0126\n",
      "   ↪ Batch 200/8000 | Loss: 0.0012\n",
      "   ↪ Batch 300/8000 | Loss: 0.0046\n",
      "   ↪ Batch 400/8000 | Loss: 0.0141\n",
      "   ↪ Batch 500/8000 | Loss: 0.0142\n",
      "   ↪ Batch 600/8000 | Loss: 0.0066\n",
      "   ↪ Batch 700/8000 | Loss: 0.0099\n",
      "   ↪ Batch 800/8000 | Loss: 0.0005\n",
      "   ↪ Batch 900/8000 | Loss: 0.0016\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0159\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0068\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0219\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0002\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0111\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0090\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0038\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0175\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0031\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0429\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0005\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0023\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0252\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0107\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0044\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0054\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0066\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0152\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0031\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0088\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0248\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0007\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0034\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0072\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0079\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0799\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0091\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0172\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0005\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0135\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0003\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0003\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0087\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0068\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0066\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0142\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0035\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0061\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0121\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0055\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0154\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0139\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0004\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0326\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0037\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0096\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0100\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0010\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0066\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0177\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0006\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0027\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0134\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0083\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0095\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0002\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0009\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0204\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0001\n",
      "✅ Epoch 5 complete | Loss: 63.1184 | Val Dice: 0.9908\n",
      "⚠️ No improvement. Patience: 1/10\n",
      "\n",
      "🔁 Epoch 6/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0205\n",
      "   ↪ Batch 100/8000 | Loss: 0.0107\n",
      "   ↪ Batch 200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 500/8000 | Loss: 0.0068\n",
      "   ↪ Batch 600/8000 | Loss: 0.0094\n",
      "   ↪ Batch 700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0162\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0211\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0018\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0069\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0056\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0265\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0004\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0102\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0214\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0222\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0036\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0527\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0043\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0116\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0320\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0027\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0022\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0127\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0087\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0169\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0030\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0107\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0002\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0059\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0086\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0028\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0004\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0053\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0179\n",
      "   ↪ Batch 4500/8000 | Loss: 0.1016\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0033\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0074\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0158\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0010\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0095\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0109\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0023\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0067\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0060\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0055\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0002\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0081\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0296\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0057\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0061\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0004\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0050\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0739\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0150\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0086\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0589\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0093\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0155\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0014\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0217\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0002\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0023\n",
      "✅ Epoch 6 complete | Loss: 60.1126 | Val Dice: 0.9930\n",
      "⚠️ No improvement. Patience: 2/10\n",
      "\n",
      "🔁 Epoch 7/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0001\n",
      "   ↪ Batch 100/8000 | Loss: 0.0179\n",
      "   ↪ Batch 200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 300/8000 | Loss: 0.0096\n",
      "   ↪ Batch 400/8000 | Loss: 0.0024\n",
      "   ↪ Batch 500/8000 | Loss: 0.0002\n",
      "   ↪ Batch 600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 800/8000 | Loss: 0.0047\n",
      "   ↪ Batch 900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0014\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0046\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0021\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0195\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0045\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0271\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0105\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0011\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0116\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0233\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0300\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0022\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0150\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0087\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0256\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0020\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0006\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0063\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0003\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0024\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0036\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0020\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0030\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0067\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0045\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0102\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0017\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0124\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0025\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0021\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0138\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0039\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0519\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0029\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0047\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0097\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0006\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0034\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0191\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0005\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0053\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0100\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0167\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0127\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0028\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0028\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0083\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0026\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0009\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0044\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0091\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0098\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0033\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0107\n",
      "✅ Epoch 7 complete | Loss: 57.8774 | Val Dice: 0.9937\n",
      "💾 Best model saved with Dice: 0.9937\n",
      "\n",
      "🔁 Epoch 8/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0017\n",
      "   ↪ Batch 100/8000 | Loss: 0.0003\n",
      "   ↪ Batch 200/8000 | Loss: 0.0154\n",
      "   ↪ Batch 300/8000 | Loss: 0.0205\n",
      "   ↪ Batch 400/8000 | Loss: 0.0222\n",
      "   ↪ Batch 500/8000 | Loss: 0.0018\n",
      "   ↪ Batch 600/8000 | Loss: 0.0042\n",
      "   ↪ Batch 700/8000 | Loss: 0.0167\n",
      "   ↪ Batch 800/8000 | Loss: 0.0007\n",
      "   ↪ Batch 900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0149\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0068\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0354\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0009\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0059\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0005\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0069\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0079\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0027\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0006\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0091\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0258\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0083\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0187\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0090\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0014\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0146\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0060\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0025\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0024\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0090\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0028\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0227\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0061\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0107\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0015\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0026\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0084\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0018\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0068\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0077\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0069\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0108\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0068\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0075\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0156\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0032\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0020\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0093\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0002\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0078\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0191\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0082\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0052\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0025\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0034\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0017\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0201\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0113\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0264\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0207\n",
      "✅ Epoch 8 complete | Loss: 56.1058 | Val Dice: 0.9915\n",
      "⚠️ No improvement. Patience: 1/10\n",
      "\n",
      "🔁 Epoch 9/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0001\n",
      "   ↪ Batch 100/8000 | Loss: 0.0042\n",
      "   ↪ Batch 200/8000 | Loss: 0.0138\n",
      "   ↪ Batch 300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 400/8000 | Loss: 0.0083\n",
      "   ↪ Batch 500/8000 | Loss: 0.0201\n",
      "   ↪ Batch 600/8000 | Loss: 0.0103\n",
      "   ↪ Batch 700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 800/8000 | Loss: 0.0148\n",
      "   ↪ Batch 900/8000 | Loss: 0.0121\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0175\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0089\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0019\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0211\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0009\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0024\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0153\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0025\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0017\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0002\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0009\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0070\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0017\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0056\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0170\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0043\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0012\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0221\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0028\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0370\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0097\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0002\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0012\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0011\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0005\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0128\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0004\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0011\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0129\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0085\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0088\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0082\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0109\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0009\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0064\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0065\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0013\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0184\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0013\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0049\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0083\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0013\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0075\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0928\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0010\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0002\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0008\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0602\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0824\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0317\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0081\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0010\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0054\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0020\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0060\n",
      "✅ Epoch 9 complete | Loss: 54.3923 | Val Dice: 0.9872\n",
      "⚠️ No improvement. Patience: 2/10\n",
      "\n",
      "🔁 Epoch 10/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0131\n",
      "   ↪ Batch 100/8000 | Loss: 0.0058\n",
      "   ↪ Batch 200/8000 | Loss: 0.0038\n",
      "   ↪ Batch 300/8000 | Loss: 0.0099\n",
      "   ↪ Batch 400/8000 | Loss: 0.0072\n",
      "   ↪ Batch 500/8000 | Loss: 0.0027\n",
      "   ↪ Batch 600/8000 | Loss: 0.0025\n",
      "   ↪ Batch 700/8000 | Loss: 0.0004\n",
      "   ↪ Batch 800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 900/8000 | Loss: 0.0016\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0412\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0003\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0040\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0381\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0083\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0002\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0025\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0016\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0018\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0067\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0026\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0009\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0092\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0101\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0059\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0070\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0011\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0074\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0082\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0064\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0412\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0026\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0039\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0035\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0107\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0011\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0045\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0052\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0053\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0213\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0020\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0040\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0022\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0065\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0031\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0025\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0031\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0091\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0047\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0041\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0176\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0275\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0057\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0127\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0026\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0066\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0074\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0011\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0042\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0142\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0026\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0210\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0003\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0000\n",
      "✅ Epoch 10 complete | Loss: 52.7083 | Val Dice: 0.9939\n",
      "💾 Best model saved with Dice: 0.9939\n",
      "\n",
      "🔁 Epoch 11/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0062\n",
      "   ↪ Batch 100/8000 | Loss: 0.0070\n",
      "   ↪ Batch 200/8000 | Loss: 0.0210\n",
      "   ↪ Batch 300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 600/8000 | Loss: 0.0002\n",
      "   ↪ Batch 700/8000 | Loss: 0.0054\n",
      "   ↪ Batch 800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 900/8000 | Loss: 0.0109\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0008\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0077\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0170\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0096\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0300\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0163\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0033\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0071\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0015\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0020\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0027\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0061\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0016\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0019\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0417\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0064\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0356\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0108\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0113\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0039\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0034\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0052\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0133\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0075\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0015\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0036\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0186\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0004\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0057\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0010\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0084\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0027\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0059\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0228\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0044\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0021\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0047\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0102\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0032\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0004\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0007\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0003\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0005\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0250\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0034\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0028\n",
      "✅ Epoch 11 complete | Loss: 51.3276 | Val Dice: 0.9940\n",
      "💾 Best model saved with Dice: 0.9940\n",
      "\n",
      "🔁 Epoch 12/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0586\n",
      "   ↪ Batch 100/8000 | Loss: 0.0013\n",
      "   ↪ Batch 200/8000 | Loss: 0.0177\n",
      "   ↪ Batch 300/8000 | Loss: 0.0002\n",
      "   ↪ Batch 400/8000 | Loss: 0.0107\n",
      "   ↪ Batch 500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 600/8000 | Loss: 0.0071\n",
      "   ↪ Batch 700/8000 | Loss: 0.0046\n",
      "   ↪ Batch 800/8000 | Loss: 0.0182\n",
      "   ↪ Batch 900/8000 | Loss: 0.0022\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0394\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0088\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0021\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0050\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0032\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0651\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0012\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0020\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0075\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0038\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0038\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0189\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0107\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0020\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0058\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0014\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0090\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0004\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0051\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0003\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0011\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0071\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0053\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0023\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0059\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0017\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0140\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0099\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0059\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0072\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0060\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0212\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0233\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0055\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0225\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0030\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0593\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0035\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0072\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0010\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0071\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0412\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0054\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0069\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0128\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0166\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0113\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0003\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0045\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0184\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0018\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0049\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0221\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0077\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0008\n",
      "✅ Epoch 12 complete | Loss: 50.4946 | Val Dice: 0.9933\n",
      "⚠️ No improvement. Patience: 1/10\n",
      "\n",
      "🔁 Epoch 13/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0001\n",
      "   ↪ Batch 100/8000 | Loss: 0.0205\n",
      "   ↪ Batch 200/8000 | Loss: 0.0005\n",
      "   ↪ Batch 300/8000 | Loss: 0.0100\n",
      "   ↪ Batch 400/8000 | Loss: 0.0077\n",
      "   ↪ Batch 500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 600/8000 | Loss: 0.0056\n",
      "   ↪ Batch 700/8000 | Loss: 0.0002\n",
      "   ↪ Batch 800/8000 | Loss: 0.0089\n",
      "   ↪ Batch 900/8000 | Loss: 0.0049\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0011\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0085\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0221\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0033\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0328\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0315\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0003\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0042\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0019\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0013\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0103\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0028\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0029\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0043\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0074\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0011\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0018\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0074\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0019\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0078\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0021\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0057\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0006\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0034\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0111\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0046\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0066\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0085\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0020\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0019\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0013\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0038\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0056\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0134\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0031\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0104\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0003\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0008\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0152\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0064\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0031\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0202\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0104\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0053\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0035\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0006\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0002\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0009\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0784\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0093\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0011\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0199\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0048\n",
      "✅ Epoch 13 complete | Loss: 49.9509 | Val Dice: 0.9930\n",
      "⚠️ No improvement. Patience: 2/10\n",
      "\n",
      "🔁 Epoch 14/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0064\n",
      "   ↪ Batch 100/8000 | Loss: 0.0017\n",
      "   ↪ Batch 200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 300/8000 | Loss: 0.0009\n",
      "   ↪ Batch 400/8000 | Loss: 0.0115\n",
      "   ↪ Batch 500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 600/8000 | Loss: 0.0004\n",
      "   ↪ Batch 700/8000 | Loss: 0.0478\n",
      "   ↪ Batch 800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 900/8000 | Loss: 0.0024\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0209\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0036\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0237\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0002\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0148\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0013\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0020\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0088\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0026\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0063\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0014\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0051\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0002\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0009\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0057\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0105\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0055\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0176\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0181\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0090\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0328\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0075\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0087\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0022\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0166\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0004\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0018\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0089\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0100\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0021\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0195\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0121\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0014\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0002\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0224\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0005\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0024\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0009\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0010\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0643\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0542\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0006\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0051\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0133\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0053\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0044\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0255\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0014\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0021\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0007\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0028\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0044\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0099\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0151\n",
      "✅ Epoch 14 complete | Loss: 48.7169 | Val Dice: 0.9934\n",
      "⚠️ No improvement. Patience: 3/10\n",
      "\n",
      "🔁 Epoch 15/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0002\n",
      "   ↪ Batch 100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 400/8000 | Loss: 0.0032\n",
      "   ↪ Batch 500/8000 | Loss: 0.0153\n",
      "   ↪ Batch 600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 700/8000 | Loss: 0.0073\n",
      "   ↪ Batch 800/8000 | Loss: 0.0001\n",
      "   ↪ Batch 900/8000 | Loss: 0.0012\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0289\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0003\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0082\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0105\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0024\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0003\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0094\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0425\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0071\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0035\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0014\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0225\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0126\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0066\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0088\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0071\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0035\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0097\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0004\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0025\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0021\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0103\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0130\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0051\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0022\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0059\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0084\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0002\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0076\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0007\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0032\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0055\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0025\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0024\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0007\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0010\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0049\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0067\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0046\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0012\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0188\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0060\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0055\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0008\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0127\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0138\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0036\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0136\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0053\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0053\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0106\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0081\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0051\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0091\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0053\n",
      "✅ Epoch 15 complete | Loss: 47.3776 | Val Dice: 0.9932\n",
      "⚠️ No improvement. Patience: 4/10\n",
      "\n",
      "🔁 Epoch 16/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0019\n",
      "   ↪ Batch 100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 400/8000 | Loss: 0.0541\n",
      "   ↪ Batch 500/8000 | Loss: 0.0001\n",
      "   ↪ Batch 600/8000 | Loss: 0.0003\n",
      "   ↪ Batch 700/8000 | Loss: 0.0073\n",
      "   ↪ Batch 800/8000 | Loss: 0.0054\n",
      "   ↪ Batch 900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 1100/8000 | Loss: 0.0097\n",
      "   ↪ Batch 1200/8000 | Loss: 0.0075\n",
      "   ↪ Batch 1300/8000 | Loss: 0.0000\n",
      "   ↪ Batch 1400/8000 | Loss: 0.0004\n",
      "   ↪ Batch 1500/8000 | Loss: 0.0024\n",
      "   ↪ Batch 1600/8000 | Loss: 0.0139\n",
      "   ↪ Batch 1700/8000 | Loss: 0.0002\n",
      "   ↪ Batch 1800/8000 | Loss: 0.0149\n",
      "   ↪ Batch 1900/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2100/8000 | Loss: 0.0152\n",
      "   ↪ Batch 2200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 2400/8000 | Loss: 0.0005\n",
      "   ↪ Batch 2500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 2600/8000 | Loss: 0.0022\n",
      "   ↪ Batch 2700/8000 | Loss: 0.0034\n",
      "   ↪ Batch 2800/8000 | Loss: 0.0164\n",
      "   ↪ Batch 2900/8000 | Loss: 0.0050\n",
      "   ↪ Batch 3000/8000 | Loss: 0.0037\n",
      "   ↪ Batch 3100/8000 | Loss: 0.0017\n",
      "   ↪ Batch 3200/8000 | Loss: 0.0014\n",
      "   ↪ Batch 3300/8000 | Loss: 0.0019\n",
      "   ↪ Batch 3400/8000 | Loss: 0.0001\n",
      "   ↪ Batch 3500/8000 | Loss: 0.0021\n",
      "   ↪ Batch 3600/8000 | Loss: 0.0006\n",
      "   ↪ Batch 3700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 3800/8000 | Loss: 0.0108\n",
      "   ↪ Batch 3900/8000 | Loss: 0.0076\n",
      "   ↪ Batch 4000/8000 | Loss: 0.0041\n",
      "   ↪ Batch 4100/8000 | Loss: 0.0082\n",
      "   ↪ Batch 4200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 4300/8000 | Loss: 0.0094\n",
      "   ↪ Batch 4400/8000 | Loss: 0.0043\n",
      "   ↪ Batch 4500/8000 | Loss: 0.0025\n",
      "   ↪ Batch 4600/8000 | Loss: 0.0216\n",
      "   ↪ Batch 4700/8000 | Loss: 0.0134\n",
      "   ↪ Batch 4800/8000 | Loss: 0.0061\n",
      "   ↪ Batch 4900/8000 | Loss: 0.0006\n",
      "   ↪ Batch 5000/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5200/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5300/8000 | Loss: 0.0005\n",
      "   ↪ Batch 5400/8000 | Loss: 0.0062\n",
      "   ↪ Batch 5500/8000 | Loss: 0.0059\n",
      "   ↪ Batch 5600/8000 | Loss: 0.0001\n",
      "   ↪ Batch 5700/8000 | Loss: 0.0152\n",
      "   ↪ Batch 5800/8000 | Loss: 0.0008\n",
      "   ↪ Batch 5900/8000 | Loss: 0.0056\n",
      "   ↪ Batch 6000/8000 | Loss: 0.0077\n",
      "   ↪ Batch 6100/8000 | Loss: 0.0002\n",
      "   ↪ Batch 6200/8000 | Loss: 0.0520\n",
      "   ↪ Batch 6300/8000 | Loss: 0.0139\n",
      "   ↪ Batch 6400/8000 | Loss: 0.0009\n",
      "   ↪ Batch 6500/8000 | Loss: 0.0256\n",
      "   ↪ Batch 6600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6700/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 6900/8000 | Loss: 0.0071\n",
      "   ↪ Batch 7000/8000 | Loss: 0.0127\n",
      "   ↪ Batch 7100/8000 | Loss: 0.0094\n",
      "   ↪ Batch 7200/8000 | Loss: 0.0074\n",
      "   ↪ Batch 7300/8000 | Loss: 0.0042\n",
      "   ↪ Batch 7400/8000 | Loss: 0.0137\n",
      "   ↪ Batch 7500/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7600/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7700/8000 | Loss: 0.0015\n",
      "   ↪ Batch 7800/8000 | Loss: 0.0000\n",
      "   ↪ Batch 7900/8000 | Loss: 0.0011\n",
      "✅ Epoch 16 complete | Loss: 47.0088 | Val Dice: 0.9937\n",
      "⚠️ No improvement. Patience: 5/10\n",
      "\n",
      "🔁 Epoch 17/100\n",
      "   ↪ Batch 0/8000 | Loss: 0.0041\n",
      "   ↪ Batch 100/8000 | Loss: 0.0001\n",
      "   ↪ Batch 200/8000 | Loss: 0.0078\n",
      "   ↪ Batch 300/8000 | Loss: 0.0001\n",
      "   ↪ Batch 400/8000 | Loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "unet = UNet3D()\n",
    "train_model(unet, train_loader, val_loader, name=\"unet3d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_unet = AttentionUNet3D()\n",
    "train_model(att_unet, train_loader, val_loader, name=\"att_unet3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d398cba",
   "metadata": {},
   "source": [
    "## 📊 Ensemble Prediction and Visualization\n",
    " This combines predictions from U-Net and Attention U-Net, averages the logits, and visualizes the ensemble result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86112439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ensemble_predict(x):\n",
    "    model1 = UNet3D().to(DEVICE)\n",
    "    model2 = AttentionUNet3D().to(DEVICE)\n",
    "    model1.load_state_dict(torch.load(\"unet3d_best.pth\"))\n",
    "    model2.load_state_dict(torch.load(\"att_unet3d_best.pth\"))\n",
    "    model1.eval(); model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out1 = model1(x)\n",
    "        out2 = model2(x)\n",
    "        avg_out = (out1 + out2) / 2\n",
    "        pred = torch.argmax(avg_out, dim=1)\n",
    "    return pred\n",
    "\n",
    "def visualize_ensemble(val_loader):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    pred = ensemble_predict(x)\n",
    "\n",
    "    mid_slice = x.shape[-1] // 2\n",
    "    for i in range(min(2, x.shape[0])):  # show 2 examples max\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(x[i][0, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Input (FLAIR)\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(y[i, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred[i, :, :, mid_slice].cpu(), cmap='gray')\n",
    "        plt.title(\"Ensemble Prediction\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "visualize_ensemble(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b165cb",
   "metadata": {},
   "source": [
    "## Export to TorchScript & ONNX\n",
    "This cell lets you export both U-Net and Attention U-Net models for deployment (in PyTorch or external tools like TensorRT or ONNX Runtime).\n",
    "\n",
    "✅ unet3d.pt and att_unet3d.pt → usable in PyTorch mobile or C++ inference\n",
    "\n",
    "✅ unet3d.onnx and att_unet3d.onnx → compatible with ONNX Runtime, TensorRT, OpenVINO, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "def export_model(model_class, weight_path, name=\"unet3d\", input_shape=(1, 4, 128, 128, 64)):\n",
    "    model = model_class().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    model.eval()\n",
    "\n",
    "    dummy_input = torch.randn(input_shape).to(DEVICE)\n",
    "\n",
    "    # ✅ Export to TorchScript\n",
    "    traced = torch.jit.trace(model, dummy_input)\n",
    "    traced.save(f\"{name}.pt\")\n",
    "    print(f\"✅ TorchScript saved: {name}.pt\")\n",
    "\n",
    "    # ✅ Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        f\"{name}.onnx\",\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"✅ ONNX saved: {name}.onnx\")\n",
    "\n",
    "# Export both models\n",
    "export_model(UNet3D, \"unet3d_best.pth\", name=\"unet3d\")\n",
    "export_model(AttentionUNet3D, \"att_unet3d_best.pth\", name=\"att_unet3d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d462fd4",
   "metadata": {},
   "source": [
    "#  Dice Score & Loss Tracking Plot\n",
    "We’ll modify the training function slightly to return per-epoch loss and Dice values, and then plot them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, name, epochs=100):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    best_dice = 0\n",
    "    patience = 10\n",
    "    trigger = 0\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_dice\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"[{name}] Epoch {epoch}\"):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        dice = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                with autocast():\n",
    "                    pred = torch.argmax(model(x), dim=1)\n",
    "                dice += dice_coeff(pred.cpu(), y.cpu())\n",
    "\n",
    "        avg_dice = dice / len(val_loader)\n",
    "        history[\"train_loss\"].append(total_loss)\n",
    "        history[\"val_dice\"].append(avg_dice.item())\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss: {total_loss:.4f} | Val Dice: {avg_dice:.4f}\")\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "            print(f\"✅ Saved best model with Dice: {best_dice:.4f}\")\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                print(\"⏹️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_history(history, title=\"Training History\"):\n",
    "    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_dice\"], label=\"Val Dice\", marker=\"x\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_unet = train_model(UNet3D(), train_loader, val_loader, name=\"unet3d\", epochs=50)\n",
    "plot_history(history_unet, title=\"U-Net 3D\")\n",
    "\n",
    "history_attn = train_model(AttentionUNet3D(), train_loader, val_loader, name=\"att_unet3d\", epochs=50)\n",
    "plot_history(history_attn, title=\"Attention U-Net 3D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00059ae3",
   "metadata": {},
   "source": [
    "# Inference on a New .nii.gz Patient\n",
    "This lets you run inference on a single new BraTS-format patient folder and visualize the middle slice result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d193fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def run_inference_on_patient(patient_dir, model_path, model_class, patch_depth=64):\n",
    "    flair = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_flair.nii.gz\")).get_fdata()\n",
    "    t1 = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t1.nii.gz\")).get_fdata()\n",
    "    t1ce = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t1ce.nii.gz\")).get_fdata()\n",
    "    t2 = nib.load(os.path.join(patient_dir, os.path.basename(patient_dir) + \"_t2.nii.gz\")).get_fdata()\n",
    "\n",
    "    # Stack channels: (4, H, W, D)\n",
    "    image = np.stack([flair, t1, t1ce, t2], axis=0).astype(np.float32)\n",
    "    image = torch.tensor(image).unsqueeze(0).to(DEVICE)  # (1, 4, H, W, D)\n",
    "\n",
    "    model = model_class().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prediction = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    return prediction  # shape: (H, W, D)\n",
    "\n",
    "# 🔍 Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_patient_prediction(patient_dir, prediction):\n",
    "    flair_path = os.path.join(patient_dir, os.path.basename(patient_dir) + \"_flair.nii.gz\")\n",
    "    flair = nib.load(flair_path).get_fdata()\n",
    "\n",
    "    mid_slice = flair.shape[-1] // 2\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(flair[:, :, mid_slice], cmap='gray')\n",
    "    plt.title(\"FLAIR Input\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(prediction[:, :, mid_slice], cmap='gray')\n",
    "    plt.title(\"Predicted Segmentation\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Inference on a single new patient folder\n",
    "new_patient = r\"D:\\BraTS2021_Training_Data\\BraTS2021_00005\"  # replace with your path\n",
    "pred = run_inference_on_patient(new_patient, \"unet3d_best.pth\", UNet3D)\n",
    "visualize_patient_prediction(new_patient, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bceae8",
   "metadata": {},
   "source": [
    "# 💾 Save Predicted Mask as .nii.gz\n",
    "\n",
    "This is useful when you want to visualize or process the segmentation result in tools like 3D Slicer, ITK-SNAP, or use for clinical post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb507e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_as_nifti(prediction, reference_nifti_path, output_path):\n",
    "    \"\"\"\n",
    "    Saves a 3D numpy array (prediction) as a .nii.gz file using the affine and header from a reference image.\n",
    "    \"\"\"\n",
    "    reference_nifti = nib.load(reference_nifti_path)\n",
    "    pred_nifti = nib.Nifti1Image(prediction.astype(np.uint8), affine=reference_nifti.affine, header=reference_nifti.header)\n",
    "    nib.save(pred_nifti, output_path)\n",
    "    print(f\"✅ Saved predicted mask as: {output_path}\")\n",
    "\n",
    "\n",
    "# Assuming you already have `pred` from the previous inference\n",
    "ref_img_path = os.path.join(new_patient, os.path.basename(new_patient) + \"_flair.nii.gz\")\n",
    "output_path = \"BraTS2021_00005_predicted_mask.nii.gz\"\n",
    "\n",
    "save_prediction_as_nifti(pred, ref_img_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44243c2",
   "metadata": {},
   "source": [
    "# Class Distribution Analysis\n",
    "This helps you analyze label imbalance across the segmentation masks, which is common in medical datasets like BraTS (e.g., large background, small tumor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac47869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Function to Analyze Label Frequencies:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_class_distribution(dataset, num_classes=4):\n",
    "    class_counts = np.zeros(num_classes)\n",
    "\n",
    "    for i in range(min(100, len(dataset))):  # analyze first 100 samples to save RAM\n",
    "        _, mask = dataset[i]\n",
    "        unique, counts = np.unique(mask.numpy(), return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            if u < num_classes:\n",
    "                class_counts[int(u)] += c\n",
    "\n",
    "    total = class_counts.sum()\n",
    "    percentages = 100 * class_counts / total\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}: {int(class_counts[cls])} voxels ({percentages[cls]:.2f}%)\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(num_classes), percentages)\n",
    "    plt.xticks(range(num_classes))\n",
    "    plt.title(\"Class Distribution (%)\")\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c0898",
   "metadata": {},
   "source": [
    "# Model Summary with torchinfo\n",
    "This helps visualize the architecture, layer-wise output shapes, and parameter counts for each model. Very useful for documentation and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def show_model_summary(model_class, input_shape=(1, 4, 128, 128, 64), name=\"Model\"):\n",
    "    print(f\"\\n📊 Summary for {name}\")\n",
    "    model = model_class().to(DEVICE)\n",
    "    summary(model, input_size=input_shape, depth=3, col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "    \n",
    "show_model_summary(UNet3D, name=\"U-Net 3D\")\n",
    "show_model_summary(AttentionUNet3D, name=\"Attention U-Net 3D\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26ebf2",
   "metadata": {},
   "source": [
    "## ✅ Summary & Deployment Tips\n",
    "- Best Dice model saved as `.pth`\n",
    "- Deployment-ready formats: `.pt` (TorchScript), `.onnx`\n",
    "- Use ONNX for ONNX Runtime, TensorRT, or OpenVINO.\n",
    "- Ideal for mobile, embedded, or web deployment.\n",
    "\n",
    "**Next step:** Try converting the ONNX model into TensorRT or integrating in a simple Flask demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
